{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews - Sentiment Model Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c conda-forge fastparquet scikit-learn arrow-cpp parquet-cpp pyarrow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from fastparquet import write\n",
    "from fastparquet import ParquetFile\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "import glob\n",
    "import ast \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs and Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'aws_region' :  'us-east-1',\n",
    "    'bucket_name': 'demos-amazon-reviews',\n",
    "    'prefix' : 'preprocessed_reviews_csvs', #only use this if you want to have your files in a folder \n",
    "    'index_key' : 'review_date_str',\n",
    "    'file_extension' :'.csv'\n",
    "   \n",
    "}\n",
    "\n",
    "global_vars = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Setting up the environment involves ensuring all the corret session and IAM roles are configured. We also need to ensure the correct region and bucket is made available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_env(configs, global_vars):\n",
    "    \n",
    "    sess = sagemaker.Session()\n",
    "    \n",
    "    role = get_execution_role()\n",
    "\n",
    "    AWS_REGION = configs['aws_region']\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    s3_bucket = s3.Bucket(configs['bucket_name'])\n",
    "\n",
    "    if s3_bucket.creation_date == None:\n",
    "    # create S3 bucket because it does not exist yet\n",
    "        print('Creating S3 bucket {}.'.format(bucket))\n",
    "        resp = s3.create_bucket(\n",
    "            ACL='private',\n",
    "            Bucket=bucket\n",
    "        )\n",
    "    else:\n",
    "        print('Bucket already exists')\n",
    "        \n",
    "    global_vars['role'] = role\n",
    "    global_vars['sess'] = sess\n",
    "    global_vars['s3'] = s3\n",
    "    global_vars['s3_bucket'] = s3_bucket\n",
    "    \n",
    "    return global_vars\n",
    "\n",
    "global_vars = setup_env(configs, global_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Manifest\n",
    "\n",
    "At this step, we need to create an index of all the files we're going to be using for this experiment and model building. Now, we don't want to download all of the data at once, or we're going to cause a lot of I/O activity for your Notebook Instance. \n",
    "\n",
    "What we're going to do is first create a path index to where the files live on S3. From there, we can do some sampling to get to see what the data looks like, do some basic sampling stats on the data, to get a better handle on how we should build a model, and then move to using all the data to build a robust model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_manifest(configs, global_vars):\n",
    "    \n",
    "    interval_printer_idx = 100\n",
    "    idx = 0\n",
    "    1\n",
    "    conn = global_vars['s3_bucket']\n",
    "    file_format = configs['file_extension']\n",
    "    index_key = configs['index_key']+'='\n",
    "    s3_prefix = configs['prefix']+'/'\n",
    "    manifest = []    \n",
    "    for file in conn.objects.filter(Prefix=s3_prefix):\n",
    "        path = file.key\n",
    "#         print(file)\n",
    "        if (file_format in path):\n",
    "#             print(path)\n",
    "            relative_path = path.replace(configs['prefix'],'')\n",
    "            date = relative_path.split('/')[1].replace(index_key,'')\n",
    "\n",
    "            man = {'idx':idx, 'path':relative_path, 'path_with_prefix':path, 'date':date}\n",
    "            manifest.append(man)  \n",
    "            idx += 1\n",
    "            if (idx % interval_printer_idx) == 0:\n",
    "                print('Processed {} files'.format(idx))\n",
    "    print('Training Dataset Size {}'.format(len(manifest)))\n",
    "    return manifest\n",
    "            \n",
    "manifest = create_dataset_manifest(configs, global_vars)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(configs, global_vars, entry):\n",
    "        \n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    resp = s3.select_object_content(\n",
    "        Bucket=configs['bucket_name'],\n",
    "        Key=entry['path_with_prefix'],\n",
    "        ExpressionType='SQL',\n",
    "        Expression=\"SELECT count(*) FROM s3object s\",\n",
    "        InputSerialization = {'CSV':\n",
    "                              {\"FileHeaderInfo\": \"Use\", \n",
    "                               \"AllowQuotedRecordDelimiter\": True,\n",
    "                               \"QuoteEscapeCharacter\":\"\\\\\",\n",
    "                              }, \n",
    "                              'CompressionType': 'NONE'},\n",
    "        OutputSerialization = {'CSV':{}},\n",
    "    )\n",
    "    \n",
    "    for event in resp['Payload']:\n",
    "        if 'Records' in event:\n",
    "            records = event['Records']['Payload'].decode('utf-8')\n",
    "#             print('Rows:',records)\n",
    "            return(int(records))\n",
    "    \n",
    "count_lines(configs, global_vars, manifest[240])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Manifest Stats\n",
    "\n",
    "Given that we know what the index of our manifest is partitioned by, let's do some simple stats to learn more about our manifest so we can make some informed decisions for our sampling and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_manifest_statistics(configs, global_vars, manifest):\n",
    "    \n",
    "    date_cnt = {}\n",
    "    stats = {}\n",
    "\n",
    "    for entry in manifest:\n",
    "        \n",
    "        date = entry['date']\n",
    "        cnt = count_lines(configs, global_vars, entry)\n",
    "        date_cnt[date] = cnt\n",
    "        print(date,cnt)\n",
    "\n",
    "    tmp = []\n",
    "    for date, cnt in date_cnt.items():\n",
    "        itm = {'date':date, 'files':cnt}\n",
    "        tmp.append(itm)\n",
    "    df_stats = pd.DataFrame(tmp)\n",
    "    df_stats.plot.bar(x='date', y='files', figsize=(40,10))  \n",
    "    df_stats.plot.kde()\n",
    "    print('Total folders {}'.format(df_stats.shape[0]))\n",
    "    print('Total Files {}'.format(df_stats.sum()['files']))\n",
    "    print('Date with most files {}. Files: {}'.format(df_stats.max()['date'], df_stats.max()['files']))\n",
    "    print('Date with least files {}. Files: {}'.format(df_stats.min()['date'], df_stats.min()['files']))\n",
    "    print('File Kurtosis/Skew {}/{}'.format(df_stats.kurtosis()['files'], df_stats.skew()['files']))\n",
    "\n",
    "    notes = '''\n",
    "        once we examine the distribution, we can determine how we are going to sample our manifest to get a better \n",
    "        understanding of our reviews. Remember the plot only represents the number of files in each folder. but\n",
    "        there is a linear relationship between the number of files, and the number of reviews per bin'''\n",
    "    \n",
    "    print(notes)\n",
    "    \n",
    "    return df_stats\n",
    "    \n",
    "manifest_df_stats = generate_manifest_statistics(configs,global_vars, manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sample Manifest\n",
    "\n",
    "Now we're going to generate a sample of our dataset to ensure that we get some views of how the data looks and feels across the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_dataset(configs, manifest, manifest_df_stats, sample_size_pct = 0.01, strategy = 'binned_normal'):\n",
    "    \n",
    "    dfs_sampled = []\n",
    "    index_key = configs['index_key']+'='\n",
    "\n",
    "    notes = '''\n",
    "        Based on the manifest inspection, we can now use a suitable sampling strategy in order to generate a smaller \n",
    "        manifest to work on locally.'''\n",
    "    \n",
    "    strategies = ['statified', 'random', 'clustered', 'systematic', 'binned_normal']\n",
    "    if strategy in strategies:\n",
    "        print('Using {} Strategy'.format(strategy))\n",
    "    else:\n",
    "        print('Please use one of these strategies {}'.format(strategies))\n",
    "    \n",
    "    if strategy is 'binned_normal':\n",
    "        sample_meta = {}\n",
    "        #we take a pct of each of the rows, and then use random to select within each bin\n",
    "        #workout the overall pct we need to take\n",
    "#         sample_pct = float(manifest_df_stats.shape[0] * sample_size)\n",
    "        for idx,row in manifest_df_stats.iterrows():\n",
    "            to_sample = int(row['files'] * sample_size_pct)\n",
    "            if to_sample < 1:\n",
    "                to_sample = 1\n",
    "            date = row['date']    \n",
    "            tmp = {'rows':row['files'], 'samples': to_sample, 'sampled_added':0}\n",
    "            sample_meta[date] = tmp\n",
    "            \n",
    "#         print(sample_meta)\n",
    "        #now we generate a new manifest\n",
    "        \n",
    "        sampled_manifest = []\n",
    "        for entry in manifest:\n",
    "\n",
    "            date = entry['path'].split('/')[1].replace(index_key,'')\n",
    "            #get the meta data \n",
    "            meta = sample_meta[date]\n",
    "            to_skip = meta['samples']\n",
    "            if to_skip == 1:\n",
    "                to_skip = 1\n",
    "            full_path = 's3://'+configs['bucket_name']+'/'+entry['path_with_prefix']\n",
    "            df = pd.read_csv(full_path, nrows=int(to_skip), header=0, error_bad_lines=False, escapechar=\"\\\\\")\n",
    "            print(date, df.shape)\n",
    "            dfs_sampled.append(df)\n",
    "        \n",
    "    elif strategy in strategies:\n",
    "        print('Other Stratigies will be supported Soon!')\n",
    "    \n",
    "    sampled_data = pd.concat(dfs_sampled)\n",
    "\n",
    "    print('New Dataset Length {}'.format(sampled_data.shape[0]))\n",
    "    \n",
    "    return sampled_data\n",
    "    \n",
    "sampled_data = generate_sample_dataset(configs, manifest, manifest_df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_load_sample_df_to_file(df, path = 'data', file_name_prefix='', operation='save', chunkSize = 100000):\n",
    "    \n",
    "    \n",
    "    loaded = []\n",
    "    #first split the df as it's too big probably\n",
    "    listOfDf = list()\n",
    "    if operation == 'save':\n",
    "\n",
    "        numberChunks = len(df) // chunkSize + 1\n",
    "        for i in range(numberChunks):\n",
    "            listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "            \n",
    "        for i in range(0, len(listOfDf)):\n",
    "            chunk_df = listOfDf[i]\n",
    "            df_tmp_name_prefix = '{}/{}_part_{}.pkl'.format(path, file_name_prefix, str(i))\n",
    "            chunk_df.to_pickle(df_tmp_name_prefix) \n",
    "                       \n",
    "        return df\n",
    "                       \n",
    "    if operation == 'load':\n",
    "        root_name = '{}/{}_*.pkl'.format(path, file_name_prefix)\n",
    "        files = glob.glob(root_name)\n",
    "        for fl in files:       \n",
    "            print(fl)\n",
    "            df = pd.read_pickle(fl)\n",
    "            loaded.append(df)\n",
    "                       \n",
    "        return pd.concat(loaded)\n",
    "    \n",
    "save_load_sample_df_to_file(None, path='data', file_name_prefix = 'sample_df', operation='load')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_sample_data(df):\n",
    "    \n",
    "    df['review_date']= pd.to_datetime(df['review_date']) \n",
    "    #convert date to string with format yyyy-mm\n",
    "    df['review_date_str'] = df['review_date'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    return df\n",
    "\n",
    "sampled_data = ready_sample_data(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def analyse_sample_dataset(df):\n",
    "   \n",
    "\n",
    "    print(df.columns)\n",
    "    print(df.shape)\n",
    "    print(df.describe())\n",
    "\n",
    "#     print(df.groupby(['review_date_str','product_category']).count())\n",
    "    tmps = []\n",
    "    for name,group in df.groupby(['review_date_str','product_category']):\n",
    "        unique_products = len(group['product_id'].unique().tolist())\n",
    "        products_with_multiple_reviews = group.shape[0]- unique_products\n",
    "        tmp = {'review_date': name[0],\n",
    "               'product_category': name[1], \n",
    "               'entries':group.shape[0], \n",
    "               'unique_products':unique_products,\n",
    "              'products_with_multiple_reviews':products_with_multiple_reviews }\n",
    "#         print(name, group.shape[0], unique_products)\n",
    "\n",
    "        tmps.append(tmp)\n",
    "    \n",
    "    df_counts_cat_years = pd.DataFrame(tmps)\n",
    "    df_counts_cat_years['review_date']= pd.to_datetime(df_counts_cat_years['review_date']) \n",
    "    df_counts_cat_years.groupby('product_category')['entries'].plot(legend=True, figsize=(30,20)) \n",
    "    plt.show()    \n",
    "    \n",
    "    df_counts_cat_years.groupby([df_counts_cat_years['review_date'].dt.year, df_counts_cat_years['review_date'].dt.month]).sum()['unique_products'].plot(kind='bar', legend=True, figsize=(30,20)) \n",
    "    plt.show()\n",
    "   \n",
    "    df_counts_cat_years.groupby(df_counts_cat_years['review_date'].dt.year).sum()['products_with_multiple_reviews'].plot(kind='bar',legend=True, figsize=(30,20))\n",
    "    plt.show()\n",
    "\n",
    "    df_counts_cat_years.groupby(df_counts_cat_years['review_date'].dt.year)['entries'].sum().plot(kind='bar',legend=True, figsize=(30,20))\n",
    "    plt.show()\n",
    "    \n",
    "analyse_sample_dataset(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Experimentation\n",
    "\n",
    "The purpose of this section is to perform some experimentations with different modelling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data for Modelling Purposes\n",
    "\n",
    "We're going to develop some dataframes which represent our Xs and Ys (features and labels).\n",
    "\n",
    "Let's create some feature/label datasets which are shaped around the following labels:\n",
    "\n",
    "-year_product-category\n",
    "-year_product-category_star_rating\n",
    "\n",
    "The features for this model will be only using the text of the reviews\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995_Books\n",
      "1996_Books\n",
      "1997_Books\n",
      "1997_Music\n",
      "1997_Video\n",
      "1997_Video_DVD\n",
      "1998_Books\n",
      "1998_Music\n",
      "1998_Video\n",
      "1998_Video_DVD\n",
      "1999_Automotive\n",
      "1999_Books\n",
      "1999_Camera\n",
      "1999_Electronics\n",
      "1999_Home_Entertainment\n",
      "1999_Home_Improvement\n",
      "1999_Music\n",
      "1999_Office_Products\n",
      "1999_PC\n",
      "1999_Software\n",
      "1999_Tools\n",
      "1999_Toys\n",
      "1999_Video\n",
      "1999_Video_DVD\n",
      "1999_Video_Games\n",
      "1999_Wireless\n",
      "2000_86\n",
      "2000_Baby\n",
      "2000_Books\n",
      "2000_Camera\n",
      "2000_Electronics\n",
      "2000_Home\n",
      "2000_Home_Entertainment\n",
      "2000_Home_Improvement\n",
      "2000_Kitchen\n",
      "2000_Lawn_and_Garden\n",
      "2000_Music\n",
      "2000_Musical_Instruments\n",
      "2000_Office_Products\n",
      "2000_Outdoors\n",
      "2000_PC\n",
      "2000_Software\n",
      "2000_Sports\n",
      "2000_Tools\n",
      "2000_Toys\n",
      "2000_Video\n",
      "2000_Video_DVD\n",
      "2000_Video_Games\n",
      "2000_Wireless\n",
      "2001_Automotive\n",
      "2001_Baby\n",
      "2001_Beauty\n",
      "2001_Books\n",
      "2001_Camera\n",
      "2001_Electronics\n",
      "2001_Grocery\n",
      "2001_Health_&_Personal_Care\n",
      "2001_Home\n",
      "2001_Home_Entertainment\n",
      "2001_Home_Improvement\n",
      "2001_Kitchen\n",
      "2001_Lawn_and_Garden\n",
      "2001_Music\n",
      "2001_Musical_Instruments\n",
      "2001_Office_Products\n",
      "2001_Outdoors\n",
      "2001_PC\n",
      "2001_Personal_Care_Appliances\n",
      "2001_Software\n",
      "2001_Sports\n",
      "2001_Tools\n",
      "2001_Toys\n",
      "2001_Video\n",
      "2001_Video_DVD\n",
      "2001_Video_Games\n",
      "2001_Wireless\n",
      "2002_Apparel\n",
      "2002_Automotive\n",
      "2002_Baby\n",
      "2002_Beauty\n",
      "2002_Books\n",
      "2002_Camera\n",
      "2002_Electronics\n",
      "2002_Health_&_Personal_Care\n",
      "2002_Home\n",
      "2002_Home_Entertainment\n",
      "2002_Home_Improvement\n",
      "2002_Jewelry\n",
      "2002_Kitchen\n",
      "2002_Lawn_and_Garden\n",
      "2002_Music\n",
      "2002_Office_Products\n",
      "2002_PC\n",
      "2002_Personal_Care_Appliances\n",
      "2002_Pet_Products\n",
      "2002_Shoes\n",
      "2002_Software\n",
      "2002_Sports\n",
      "2002_Tools\n",
      "2002_Toys\n",
      "2002_Video\n",
      "2002_Video_DVD\n",
      "2002_Video_Games\n",
      "2002_Wireless\n",
      "2003_Apparel\n",
      "2003_Baby\n",
      "2003_Beauty\n",
      "2003_Books\n",
      "2003_Camera\n",
      "2003_Digital_Ebook_Purchase\n",
      "2003_Electronics\n",
      "2003_Furniture\n",
      "2003_Grocery\n",
      "2003_Health_&_Personal_Care\n",
      "2003_Home\n",
      "2003_Home_Entertainment\n",
      "2003_Home_Improvement\n",
      "2003_Jewelry\n",
      "2003_Kitchen\n",
      "2003_Lawn_and_Garden\n",
      "2003_Luggage\n",
      "2003_Music\n",
      "2003_Musical_Instruments\n",
      "2003_Office_Products\n",
      "2003_Outdoors\n",
      "2003_PC\n",
      "2003_Pet_Products\n",
      "2003_Shoes\n",
      "2003_Software\n",
      "2003_Sports\n",
      "2003_Tools\n",
      "2003_Toys\n",
      "2003_Video\n",
      "2003_Video_DVD\n",
      "2003_Video_Games\n",
      "2003_Watches\n",
      "2003_Wireless\n",
      "2004_Apparel\n",
      "2004_Automotive\n",
      "2004_Baby\n",
      "2004_Beauty\n",
      "2004_Books\n",
      "2004_Camera\n",
      "2004_Electronics\n",
      "2004_Furniture\n",
      "2004_Grocery\n",
      "2004_Health_&_Personal_Care\n",
      "2004_Home\n",
      "2004_Home_Entertainment\n",
      "2004_Home_Improvement\n",
      "2004_Jewelry\n",
      "2004_Kitchen\n",
      "2004_Lawn_and_Garden\n",
      "2004_Major_Appliances\n",
      "2004_Music\n",
      "2004_Musical_Instruments\n",
      "2004_Office_Products\n",
      "2004_Outdoors\n",
      "2004_PC\n",
      "2004_Personal_Care_Appliances\n",
      "2004_Pet_Products\n",
      "2004_Shoes\n",
      "2004_Software\n",
      "2004_Sports\n",
      "2004_Tools\n",
      "2004_Toys\n",
      "2004_Video\n",
      "2004_Video_DVD\n",
      "2004_Video_Games\n",
      "2004_Watches\n",
      "2004_Wireless\n",
      "2005_Apparel\n",
      "2005_Automotive\n",
      "2005_Baby\n",
      "2005_Beauty\n",
      "2005_Books\n",
      "2005_Camera\n",
      "2005_Digital_Video_Download\n",
      "2005_Electronics\n",
      "2005_Furniture\n",
      "2005_Grocery\n",
      "2005_Health_&_Personal_Care\n",
      "2005_Home\n",
      "2005_Home_Entertainment\n",
      "2005_Home_Improvement\n",
      "2005_Jewelry\n",
      "2005_Kitchen\n",
      "2005_Lawn_and_Garden\n",
      "2005_Luggage\n",
      "2005_Major_Appliances\n",
      "2005_Mobile_Electronics\n",
      "2005_Music\n",
      "2005_Musical_Instruments\n",
      "2005_Office_Products\n",
      "2005_Outdoors\n",
      "2005_PC\n",
      "2005_Personal_Care_Appliances\n",
      "2005_Pet_Products\n",
      "2005_Shoes\n",
      "2005_Software\n",
      "2005_Sports\n",
      "2005_Tools\n",
      "2005_Toys\n",
      "2005_Video\n",
      "2005_Video_DVD\n",
      "2005_Video_Games\n",
      "2005_Watches\n",
      "2005_Wireless\n",
      "2006_Apparel\n",
      "2006_Automotive\n",
      "2006_Baby\n",
      "2006_Beauty\n",
      "2006_Books\n",
      "2006_Camera\n",
      "2006_Digital_Video_Download\n",
      "2006_Electronics\n",
      "2006_Furniture\n",
      "2006_Grocery\n",
      "2006_Health_&_Personal_Care\n",
      "2006_Home\n",
      "2006_Home_Entertainment\n",
      "2006_Home_Improvement\n",
      "2006_Jewelry\n",
      "2006_Kitchen\n",
      "2006_Lawn_and_Garden\n",
      "2006_Luggage\n",
      "2006_Major_Appliances\n",
      "2006_Mobile_Electronics\n",
      "2006_Music\n",
      "2006_Musical_Instruments\n",
      "2006_Office_Products\n",
      "2006_Outdoors\n",
      "2006_PC\n",
      "2006_Personal_Care_Appliances\n",
      "2006_Pet_Products\n",
      "2006_Shoes\n",
      "2006_Software\n",
      "2006_Sports\n",
      "2006_Tools\n",
      "2006_Toys\n",
      "2006_Video\n",
      "2006_Video_DVD\n",
      "2006_Video_Games\n",
      "2006_Watches\n",
      "2006_Wireless\n",
      "2007_Apparel\n",
      "2007_Automotive\n",
      "2007_Baby\n",
      "2007_Beauty\n",
      "2007_Books\n",
      "2007_Camera\n",
      "2007_Digital_Ebook_Purchase\n",
      "2007_Digital_Music_Purchase\n",
      "2007_Digital_Video_Download\n",
      "2007_Electronics\n",
      "2007_Furniture\n",
      "2007_Gift_Card\n",
      "2007_Grocery\n",
      "2007_Health_&_Personal_Care\n",
      "2007_Home\n",
      "2007_Home_Entertainment\n",
      "2007_Home_Improvement\n",
      "2007_Jewelry\n",
      "2007_Kitchen\n",
      "2007_Lawn_and_Garden\n",
      "2007_Luggage\n",
      "2007_Major_Appliances\n",
      "2007_Mobile_Electronics\n",
      "2007_Music\n",
      "2007_Musical_Instruments\n",
      "2007_Office_Products\n",
      "2007_Outdoors\n",
      "2007_PC\n",
      "2007_Personal_Care_Appliances\n",
      "2007_Pet_Products\n",
      "2007_Shoes\n",
      "2007_Software\n",
      "2007_Sports\n",
      "2007_Tools\n",
      "2007_Toys\n",
      "2007_Video\n",
      "2007_Video_DVD\n",
      "2007_Video_Games\n",
      "2007_Watches\n",
      "2007_Wireless\n",
      "2008_Apparel\n",
      "2008_Automotive\n",
      "2008_Baby\n",
      "2008_Beauty\n",
      "2008_Books\n",
      "2008_Camera\n",
      "2008_Digital_Ebook_Purchase\n",
      "2008_Digital_Music_Purchase\n",
      "2008_Digital_Video_Download\n",
      "2008_Digital_Video_Games\n",
      "2008_Electronics\n",
      "2008_Furniture\n",
      "2008_Gift_Card\n",
      "2008_Grocery\n",
      "2008_Health_&_Personal_Care\n",
      "2008_Home\n",
      "2008_Home_Entertainment\n",
      "2008_Home_Improvement\n",
      "2008_Jewelry\n",
      "2008_Kitchen\n",
      "2008_Lawn_and_Garden\n",
      "2008_Luggage\n",
      "2008_Major_Appliances\n",
      "2008_Mobile_Electronics\n",
      "2008_Music\n",
      "2008_Musical_Instruments\n",
      "2008_Office_Products\n",
      "2008_Outdoors\n",
      "2008_PC\n",
      "2008_Personal_Care_Appliances\n",
      "2008_Pet_Products\n",
      "2008_Shoes\n",
      "2008_Software\n",
      "2008_Sports\n",
      "2008_Tools\n",
      "2008_Toys\n",
      "2008_Video\n",
      "2008_Video_DVD\n",
      "2008_Video_Games\n",
      "2008_Watches\n",
      "2008_Wireless\n",
      "2009_87\n",
      "2009_Apparel\n",
      "2009_Automotive\n",
      "2009_Baby\n",
      "2009_Beauty\n",
      "2009_Books\n",
      "2009_Camera\n",
      "2009_Digital_Ebook_Purchase\n",
      "2009_Digital_Music_Purchase\n",
      "2009_Digital_Software\n",
      "2009_Digital_Video_Download\n",
      "2009_Digital_Video_Games\n",
      "2009_Electronics\n",
      "2009_Furniture\n",
      "2009_Gift_Card\n",
      "2009_Grocery\n",
      "2009_Health_&_Personal_Care\n",
      "2009_Home\n",
      "2009_Home_Entertainment\n",
      "2009_Home_Improvement\n",
      "2009_Jewelry\n",
      "2009_Kitchen\n",
      "2009_Lawn_and_Garden\n",
      "2009_Luggage\n",
      "2009_Major_Appliances\n",
      "2009_Mobile_Electronics\n",
      "2009_Music\n",
      "2009_Musical_Instruments\n",
      "2009_Office_Products\n",
      "2009_Outdoors\n",
      "2009_PC\n",
      "2009_Personal_Care_Appliances\n",
      "2009_Pet_Products\n",
      "2009_Shoes\n",
      "2009_Software\n",
      "2009_Sports\n",
      "2009_Tools\n",
      "2009_Toys\n",
      "2009_Video\n",
      "2009_Video_DVD\n",
      "2009_Video_Games\n",
      "2009_Watches\n",
      "2009_Wireless\n",
      "2010_281\n",
      "2010_Apparel\n",
      "2010_Automotive\n",
      "2010_Baby\n",
      "2010_Beauty\n",
      "2010_Books\n",
      "2010_Camera\n",
      "2010_Digital_Ebook_Purchase\n",
      "2010_Digital_Music_Purchase\n",
      "2010_Digital_Software\n",
      "2010_Digital_Video_Download\n",
      "2010_Digital_Video_Games\n",
      "2010_Electronics\n",
      "2010_Furniture\n",
      "2010_Gift_Card\n",
      "2010_Grocery\n",
      "2010_Health_&_Personal_Care\n",
      "2010_Home\n",
      "2010_Home_Entertainment\n",
      "2010_Home_Improvement\n",
      "2010_Jewelry\n",
      "2010_Kitchen\n",
      "2010_Lawn_and_Garden\n",
      "2010_Luggage\n",
      "2010_Major_Appliances\n",
      "2010_Mobile_Electronics\n",
      "2010_Music\n",
      "2010_Musical_Instruments\n",
      "2010_Office_Products\n",
      "2010_Outdoors\n",
      "2010_PC\n",
      "2010_Personal_Care_Appliances\n",
      "2010_Pet_Products\n",
      "2010_Shoes\n",
      "2010_Software\n",
      "2010_Sports\n",
      "2010_Tools\n",
      "2010_Toys\n",
      "2010_Video\n",
      "2010_Video_DVD\n",
      "2010_Video_Games\n",
      "2010_Watches\n",
      "2010_Wireless\n",
      "2011_146\n",
      "2011_165\n",
      "2011_Apparel\n",
      "2011_Automotive\n",
      "2011_Baby\n",
      "2011_Beauty\n",
      "2011_Books\n",
      "2011_Camera\n",
      "2011_Digital_Ebook_Purchase\n",
      "2011_Digital_Music_Purchase\n",
      "2011_Digital_Software\n",
      "2011_Digital_Video_Download\n",
      "2011_Digital_Video_Games\n",
      "2011_Electronics\n",
      "2011_Furniture\n",
      "2011_Gift_Card\n",
      "2011_Grocery\n",
      "2011_Health_&_Personal_Care\n",
      "2011_Home\n",
      "2011_Home_Entertainment\n",
      "2011_Home_Improvement\n",
      "2011_Jewelry\n",
      "2011_Kitchen\n",
      "2011_Lawn_and_Garden\n",
      "2011_Luggage\n",
      "2011_Major_Appliances\n",
      "2011_Mobile_Apps\n",
      "2011_Mobile_Electronics\n",
      "2011_Music\n",
      "2011_Musical_Instruments\n",
      "2011_Office_Products\n",
      "2011_Outdoors\n",
      "2011_PC\n",
      "2011_Personal_Care_Appliances\n",
      "2011_Pet_Products\n",
      "2011_Shoes\n",
      "2011_Software\n",
      "2011_Sports\n",
      "2011_Tools\n",
      "2011_Toys\n",
      "2011_Video\n",
      "2011_Video_DVD\n",
      "2011_Video_Games\n",
      "2011_Watches\n",
      "2011_Wireless\n",
      "2012_Apparel\n",
      "2012_Automotive\n",
      "2012_Baby\n",
      "2012_Beauty\n",
      "2012_Books\n",
      "2012_Camera\n",
      "2012_Digital_Ebook_Purchase\n",
      "2012_Digital_Music_Purchase\n",
      "2012_Digital_Software\n",
      "2012_Digital_Video_Download\n",
      "2012_Digital_Video_Games\n",
      "2012_Electronics\n",
      "2012_Furniture\n",
      "2012_Gift_Card\n",
      "2012_Grocery\n",
      "2012_Health_&_Personal_Care\n",
      "2012_Home\n",
      "2012_Home_Entertainment\n",
      "2012_Home_Improvement\n",
      "2012_Jewelry\n",
      "2012_Kitchen\n",
      "2012_Lawn_and_Garden\n",
      "2012_Luggage\n",
      "2012_Major_Appliances\n",
      "2012_Mobile_Apps\n",
      "2012_Mobile_Electronics\n",
      "2012_Music\n",
      "2012_Musical_Instruments\n",
      "2012_Office_Products\n",
      "2012_Outdoors\n",
      "2012_PC\n",
      "2012_Personal_Care_Appliances\n",
      "2012_Pet_Products\n",
      "2012_Shoes\n",
      "2012_Software\n",
      "2012_Sports\n",
      "2012_Tools\n",
      "2012_Toys\n",
      "2012_Video\n",
      "2012_Video_DVD\n",
      "2012_Video_Games\n",
      "2012_Watches\n",
      "2012_Wireless\n",
      "2013_106\n",
      "2013_127\n",
      "2013_577\n",
      "2013_Apparel\n",
      "2013_Automotive\n",
      "2013_Baby\n",
      "2013_Beauty\n",
      "2013_Books\n",
      "2013_Camera\n",
      "2013_Digital_Ebook_Purchase\n",
      "2013_Digital_Music_Purchase\n",
      "2013_Digital_Software\n",
      "2013_Digital_Video_Download\n",
      "2013_Digital_Video_Games\n",
      "2013_Electronics\n",
      "2013_Furniture\n",
      "2013_Gift_Card\n",
      "2013_Grocery\n",
      "2013_Health_&_Personal_Care\n",
      "2013_Home\n",
      "2013_Home_Entertainment\n",
      "2013_Home_Improvement\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013_Jewelry\n",
      "2013_Kitchen\n",
      "2013_Lawn_and_Garden\n",
      "2013_Luggage\n",
      "2013_Major_Appliances\n",
      "2013_Mobile_Apps\n",
      "2013_Mobile_Electronics\n",
      "2013_Music\n",
      "2013_Musical_Instruments\n",
      "2013_Office_Products\n",
      "2013_Outdoors\n",
      "2013_PC\n",
      "2013_Personal_Care_Appliances\n",
      "2013_Pet_Products\n",
      "2013_Shoes\n",
      "2013_Software\n",
      "2013_Sports\n",
      "2013_Tools\n",
      "2013_Toys\n",
      "2013_Video\n",
      "2013_Video_DVD\n",
      "2013_Video_Games\n",
      "2013_Watches\n",
      "2013_Wireless\n",
      "2014_104\n",
      "2014_120\n",
      "2014_236\n",
      "2014_338\n",
      "2014_Apparel\n",
      "2014_Automotive\n",
      "2014_Baby\n",
      "2014_Beauty\n",
      "2014_Books\n",
      "2014_Camera\n",
      "2014_Digital_Ebook_Purchase\n",
      "2014_Digital_Music_Purchase\n",
      "2014_Digital_Software\n",
      "2014_Digital_Video_Download\n",
      "2014_Digital_Video_Games\n",
      "2014_Electronics\n",
      "2014_Furniture\n",
      "2014_Gift_Card\n",
      "2014_Grocery\n",
      "2014_Health_&_Personal_Care\n",
      "2014_Home\n",
      "2014_Home_Entertainment\n",
      "2014_Home_Improvement\n",
      "2014_Jewelry\n",
      "2014_Kitchen\n",
      "2014_Lawn_and_Garden\n",
      "2014_Luggage\n",
      "2014_Major_Appliances\n",
      "2014_Mobile_Apps\n",
      "2014_Mobile_Electronics\n",
      "2014_Music\n",
      "2014_Musical_Instruments\n",
      "2014_Office_Products\n",
      "2014_Outdoors\n",
      "2014_PC\n",
      "2014_Personal_Care_Appliances\n",
      "2014_Pet_Products\n",
      "2014_Shoes\n",
      "2014_Software\n",
      "2014_Sports\n",
      "2014_Tools\n",
      "2014_Toys\n",
      "2014_Video\n",
      "2014_Video_DVD\n",
      "2014_Video_Games\n",
      "2014_Watches\n",
      "2014_Wireless\n",
      "2015_177\n",
      "2015_252\n",
      "2015_552\n",
      "2015_57\n",
      "2015_74\n",
      "2015_Apparel\n",
      "2015_Automotive\n",
      "2015_Baby\n",
      "2015_Beauty\n",
      "2015_Books\n",
      "2015_Camera\n",
      "2015_Digital_Ebook_Purchase\n",
      "2015_Digital_Music_Purchase\n",
      "2015_Digital_Software\n",
      "2015_Digital_Video_Download\n",
      "2015_Digital_Video_Games\n",
      "2015_Electronics\n",
      "2015_Furniture\n",
      "2015_Gift_Card\n",
      "2015_Grocery\n",
      "2015_Health_&_Personal_Care\n",
      "2015_Home\n",
      "2015_Home_Entertainment\n",
      "2015_Home_Improvement\n",
      "2015_Jewelry\n",
      "2015_Kitchen\n",
      "2015_Lawn_and_Garden\n",
      "2015_Luggage\n",
      "2015_Major_Appliances\n",
      "2015_Mobile_Apps\n",
      "2015_Mobile_Electronics\n",
      "2015_Music\n",
      "2015_Musical_Instruments\n",
      "2015_Office_Products\n",
      "2015_Outdoors\n",
      "2015_PC\n",
      "2015_Personal_Care_Appliances\n",
      "2015_Pet_Products\n",
      "2015_Shoes\n",
      "2015_Software\n",
      "2015_Sports\n",
      "2015_Tools\n",
      "2015_Toys\n",
      "2015_Video\n",
      "2015_Video_DVD\n",
      "2015_Video_Games\n",
      "2015_Watches\n",
      "2015_Wireless\n"
     ]
    }
   ],
   "source": [
    "def transform_data_for_modelling_use(df):\n",
    "    \n",
    "    #first let's get all our data in correct buckets of features and labels\n",
    "    tmps = list()\n",
    "    \n",
    "    for name,group in df.groupby([df['review_date'].dt.year,'product_category']):\n",
    "        label = '{}_{}'.format(name[0],name[1])\n",
    "        print(label)\n",
    "        tokens = list()\n",
    "        reviews = group['review_body_processed']\n",
    "        for review in reviews:\n",
    "#             print (type(review)) \n",
    "            res = str(review).strip('][').split(', ') \n",
    "            tokens.append(res)\n",
    "#         print(len(tokens))\n",
    "        tmp = {'tokens':tokens, 'label':label }\n",
    "#         print(tmp)\n",
    "        tmps.append(tmp)\n",
    "    \n",
    "    df_year_product_category = pd.DataFrame(tmps)\n",
    "    return df_year_product_category\n",
    "\n",
    "df_year_product_category = transform_data_for_modelling_use(sampled_data)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the features for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995_Books</td>\n",
       "      <td>[[nice, diags, lucid, explanations, rigging, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996_Books</td>\n",
       "      <td>[[nine, doors, midgard, excellent, sourcebook,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_Books</td>\n",
       "      <td>[[im, avid, cook, voracious, reader, cook, boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_Music</td>\n",
       "      <td>[[summers, 1992, selftitled, record, followed,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_Video</td>\n",
       "      <td>[[sad, note, americans, havent, pleasure, read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997_Video_DVD</td>\n",
       "      <td>[[bye, bye, birdie, delicious, slice, musical,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1998_Books</td>\n",
       "      <td>[[graduate, unc, rabid, fan, tar, heels, heard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1998_Music</td>\n",
       "      <td>[[cds, one, great, songs, especially, like, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998_Video</td>\n",
       "      <td>[[movie, real, look, life, many, angles, demon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1998_Video_DVD</td>\n",
       "      <td>[[marisa, tormei, plays, woman, told, fortune,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1999_Automotive</td>\n",
       "      <td>[[recently, installed, system, 2000, mitsubush...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1999_Books</td>\n",
       "      <td>[[loved, book, realy, liked, fact, andalites, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1999_Camera</td>\n",
       "      <td>[[bought, camera, never, took, pictures, margi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999_Electronics</td>\n",
       "      <td>[[525, receiver, six, months, really, like, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1999_Home_Entertainment</td>\n",
       "      <td>[[extremely, pleased, dvd, player, dvd, genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1999_Home_Improvement</td>\n",
       "      <td>[[couple, things, work, really, well, hose, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1999_Music</td>\n",
       "      <td>[[like, dance, club, athmosphere, home, car, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1999_Office_Products</td>\n",
       "      <td>[[use, vb, sql, access, excel, every, day, pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1999_PC</td>\n",
       "      <td>[[ive, owned, zip, drive, 3, years, already, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1999_Software</td>\n",
       "      <td>[[two, kids, heart, played, whole, way, thru, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1999_Tools</td>\n",
       "      <td>[[tool, greati, put, tough, darilling, amp, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1999_Toys</td>\n",
       "      <td>[[seriously, fun, toy, office, people, stop, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1999_Video</td>\n",
       "      <td>[[film, orson, welles, ever, made, within, all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1999_Video_DVD</td>\n",
       "      <td>[[months, friends, mine, talking, movie, rente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1999_Video_Games</td>\n",
       "      <td>[[tenchu, stealth, assasins, one, rare, games,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1999_Wireless</td>\n",
       "      <td>[[well, got, really, works, also, low, netherl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2000_86</td>\n",
       "      <td>[[nan]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000_Baby</td>\n",
       "      <td>[[best, thing, fishbowl, piece, played, separa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2000_Books</td>\n",
       "      <td>[[john, marks, templetons, important, discover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2000_Camera</td>\n",
       "      <td>[[first, camera, ever, owned, perfect, machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>2015_Gift_Card</td>\n",
       "      <td>[[froggy, great, job, delivered, gift, time, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>2015_Grocery</td>\n",
       "      <td>[[ok, give, energy, thought, would, flavorful,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>2015_Health_&amp;_Personal_Care</td>\n",
       "      <td>[[excellent, product], [45, days, consistently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2015_Home</td>\n",
       "      <td>[[comfortable, pillow, hard, soft, drawback, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>2015_Home_Entertainment</td>\n",
       "      <td>[[works, locke, new, thing, wifi, one, option,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2015_Home_Improvement</td>\n",
       "      <td>[[wellmade, attractive, everything, wanted, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>2015_Jewelry</td>\n",
       "      <td>[[beautiful], [nice, solid, feeling, inblue, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2015_Kitchen</td>\n",
       "      <td>[[bought, wifes, birthday, loves, purple, bonu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>2015_Lawn_and_Garden</td>\n",
       "      <td>[[great, cart, big, capacities, quiet, need, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>2015_Luggage</td>\n",
       "      <td>[[bag, ripped, 2, days, buying], [product, adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>2015_Major_Appliances</td>\n",
       "      <td>[[hate, washing, machine, ruined, pieces, clot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>2015_Mobile_Apps</td>\n",
       "      <td>[[wen, eve, enter, authenticates, tells, updat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>2015_Mobile_Electronics</td>\n",
       "      <td>[[love, couldnt, happier], [finally, find, key...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2015_Music</td>\n",
       "      <td>[[3rd, time, buying, good, cd], [great, banjo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2015_Musical_Instruments</td>\n",
       "      <td>[[ordered, machine, holiday, weekend, july, 3r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>2015_Office_Products</td>\n",
       "      <td>[[needed, complete, customers, orderbr, went, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>2015_Outdoors</td>\n",
       "      <td>[[doesnt, charge, fitbit, flex], [christmas, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2015_PC</td>\n",
       "      <td>[[wouldnt, recommend, particular, brand, mater...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2015_Personal_Care_Appliances</td>\n",
       "      <td>[[bought, use, castor, oil, packs, nice, size,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2015_Pet_Products</td>\n",
       "      <td>[[good, product], [bought, clippers, brush, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2015_Shoes</td>\n",
       "      <td>[[comfortable, fit, expected, great, shoes], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>2015_Software</td>\n",
       "      <td>[[works, great], [replace, hdd, wifes, pc, win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>2015_Sports</td>\n",
       "      <td>[[brother, uses, jacket, every, chillyrainy, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>2015_Tools</td>\n",
       "      <td>[[great, tool, wish, would, gotten, hammer, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>2015_Toys</td>\n",
       "      <td>[[worst, toy, ever, epitomizes, wrong, jw, toy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>2015_Video</td>\n",
       "      <td>[[promised], [favorite, movie, child, hood], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>2015_Video_DVD</td>\n",
       "      <td>[[grew, hilarious, movie], [loved, worth, mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>2015_Video_Games</td>\n",
       "      <td>[[didnt, see, scratches, marks, described, arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>2015_Watches</td>\n",
       "      <td>[[bought, gift, loved], [seems, next, cedar, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>2015_Wireless</td>\n",
       "      <td>[[ok, excelent, samsung, galaxy, duos, g7102],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>641 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             label  \\\n",
       "0                       1995_Books   \n",
       "1                       1996_Books   \n",
       "2                       1997_Books   \n",
       "3                       1997_Music   \n",
       "4                       1997_Video   \n",
       "5                   1997_Video_DVD   \n",
       "6                       1998_Books   \n",
       "7                       1998_Music   \n",
       "8                       1998_Video   \n",
       "9                   1998_Video_DVD   \n",
       "10                 1999_Automotive   \n",
       "11                      1999_Books   \n",
       "12                     1999_Camera   \n",
       "13                1999_Electronics   \n",
       "14         1999_Home_Entertainment   \n",
       "15           1999_Home_Improvement   \n",
       "16                      1999_Music   \n",
       "17            1999_Office_Products   \n",
       "18                         1999_PC   \n",
       "19                   1999_Software   \n",
       "20                      1999_Tools   \n",
       "21                       1999_Toys   \n",
       "22                      1999_Video   \n",
       "23                  1999_Video_DVD   \n",
       "24                1999_Video_Games   \n",
       "25                   1999_Wireless   \n",
       "26                         2000_86   \n",
       "27                       2000_Baby   \n",
       "28                      2000_Books   \n",
       "29                     2000_Camera   \n",
       "..                             ...   \n",
       "611                 2015_Gift_Card   \n",
       "612                   2015_Grocery   \n",
       "613    2015_Health_&_Personal_Care   \n",
       "614                      2015_Home   \n",
       "615        2015_Home_Entertainment   \n",
       "616          2015_Home_Improvement   \n",
       "617                   2015_Jewelry   \n",
       "618                   2015_Kitchen   \n",
       "619           2015_Lawn_and_Garden   \n",
       "620                   2015_Luggage   \n",
       "621          2015_Major_Appliances   \n",
       "622               2015_Mobile_Apps   \n",
       "623        2015_Mobile_Electronics   \n",
       "624                     2015_Music   \n",
       "625       2015_Musical_Instruments   \n",
       "626           2015_Office_Products   \n",
       "627                  2015_Outdoors   \n",
       "628                        2015_PC   \n",
       "629  2015_Personal_Care_Appliances   \n",
       "630              2015_Pet_Products   \n",
       "631                     2015_Shoes   \n",
       "632                  2015_Software   \n",
       "633                    2015_Sports   \n",
       "634                     2015_Tools   \n",
       "635                      2015_Toys   \n",
       "636                     2015_Video   \n",
       "637                 2015_Video_DVD   \n",
       "638               2015_Video_Games   \n",
       "639                   2015_Watches   \n",
       "640                  2015_Wireless   \n",
       "\n",
       "                                                tokens  \n",
       "0    [[nice, diags, lucid, explanations, rigging, g...  \n",
       "1    [[nine, doors, midgard, excellent, sourcebook,...  \n",
       "2    [[im, avid, cook, voracious, reader, cook, boo...  \n",
       "3    [[summers, 1992, selftitled, record, followed,...  \n",
       "4    [[sad, note, americans, havent, pleasure, read...  \n",
       "5    [[bye, bye, birdie, delicious, slice, musical,...  \n",
       "6    [[graduate, unc, rabid, fan, tar, heels, heard...  \n",
       "7    [[cds, one, great, songs, especially, like, si...  \n",
       "8    [[movie, real, look, life, many, angles, demon...  \n",
       "9    [[marisa, tormei, plays, woman, told, fortune,...  \n",
       "10   [[recently, installed, system, 2000, mitsubush...  \n",
       "11   [[loved, book, realy, liked, fact, andalites, ...  \n",
       "12   [[bought, camera, never, took, pictures, margi...  \n",
       "13   [[525, receiver, six, months, really, like, bu...  \n",
       "14   [[extremely, pleased, dvd, player, dvd, genera...  \n",
       "15   [[couple, things, work, really, well, hose, fa...  \n",
       "16   [[like, dance, club, athmosphere, home, car, d...  \n",
       "17   [[use, vb, sql, access, excel, every, day, pos...  \n",
       "18   [[ive, owned, zip, drive, 3, years, already, h...  \n",
       "19   [[two, kids, heart, played, whole, way, thru, ...  \n",
       "20   [[tool, greati, put, tough, darilling, amp, sc...  \n",
       "21   [[seriously, fun, toy, office, people, stop, p...  \n",
       "22   [[film, orson, welles, ever, made, within, all...  \n",
       "23   [[months, friends, mine, talking, movie, rente...  \n",
       "24   [[tenchu, stealth, assasins, one, rare, games,...  \n",
       "25   [[well, got, really, works, also, low, netherl...  \n",
       "26                                             [[nan]]  \n",
       "27   [[best, thing, fishbowl, piece, played, separa...  \n",
       "28   [[john, marks, templetons, important, discover...  \n",
       "29   [[first, camera, ever, owned, perfect, machine...  \n",
       "..                                                 ...  \n",
       "611  [[froggy, great, job, delivered, gift, time, e...  \n",
       "612  [[ok, give, energy, thought, would, flavorful,...  \n",
       "613  [[excellent, product], [45, days, consistently...  \n",
       "614  [[comfortable, pillow, hard, soft, drawback, t...  \n",
       "615  [[works, locke, new, thing, wifi, one, option,...  \n",
       "616  [[wellmade, attractive, everything, wanted, ex...  \n",
       "617  [[beautiful], [nice, solid, feeling, inblue, i...  \n",
       "618  [[bought, wifes, birthday, loves, purple, bonu...  \n",
       "619  [[great, cart, big, capacities, quiet, need, l...  \n",
       "620  [[bag, ripped, 2, days, buying], [product, adv...  \n",
       "621  [[hate, washing, machine, ruined, pieces, clot...  \n",
       "622  [[wen, eve, enter, authenticates, tells, updat...  \n",
       "623  [[love, couldnt, happier], [finally, find, key...  \n",
       "624  [[3rd, time, buying, good, cd], [great, banjo,...  \n",
       "625  [[ordered, machine, holiday, weekend, july, 3r...  \n",
       "626  [[needed, complete, customers, orderbr, went, ...  \n",
       "627  [[doesnt, charge, fitbit, flex], [christmas, g...  \n",
       "628  [[wouldnt, recommend, particular, brand, mater...  \n",
       "629  [[bought, use, castor, oil, packs, nice, size,...  \n",
       "630  [[good, product], [bought, clippers, brush, wo...  \n",
       "631  [[comfortable, fit, expected, great, shoes], [...  \n",
       "632  [[works, great], [replace, hdd, wifes, pc, win...  \n",
       "633  [[brother, uses, jacket, every, chillyrainy, d...  \n",
       "634  [[great, tool, wish, would, gotten, hammer, on...  \n",
       "635  [[worst, toy, ever, epitomizes, wrong, jw, toy...  \n",
       "636  [[promised], [favorite, movie, child, hood], [...  \n",
       "637  [[grew, hilarious, movie], [loved, worth, mone...  \n",
       "638  [[didnt, see, scratches, marks, described, arr...  \n",
       "639  [[bought, gift, loved], [seems, next, cedar, e...  \n",
       "640  [[ok, excelent, samsung, galaxy, duos, g7102],...  \n",
       "\n",
       "[641 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_load_sample_df_to_file(df_year_product_category, \n",
    "                            path='data', \n",
    "                            file_name_prefix = 'features_df_year_product_category', \n",
    "                            operation='save', chunkSize=10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - Temporal Analysis of Product Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books 193606\n",
      "Music 46377\n",
      "Video 70134\n",
      "Automotive 35293\n",
      "Camera 17565\n",
      "Electronics 31860\n",
      "Home 94934\n",
      "Office 25115\n",
      "PC 68088\n",
      "Software 3238\n",
      "Tools 17763\n",
      "Toys 50558\n",
      "Wireless 89363\n",
      "86 1\n",
      "Baby 16649\n",
      "Kitchen 46492\n",
      "Lawn 25412\n",
      "Musical 8738\n",
      "Outdoors 22518\n",
      "Sports 47688\n",
      "Beauty 50556\n",
      "Grocery 23750\n",
      "Health 52120\n",
      "Personal 807\n",
      "Apparel 61003\n",
      "Jewelry 17951\n",
      "Pet 25120\n",
      "Shoes 43825\n",
      "Digital 229243\n",
      "Furniture 7782\n",
      "Luggage 3354\n",
      "Watches 9152\n",
      "Major 1002\n",
      "Mobile 48872\n",
      "Gift 1348\n",
      "87 1\n",
      "281 1\n",
      "146 1\n",
      "165 1\n",
      "106 1\n",
      "127 1\n",
      "577 1\n",
      "104 1\n",
      "120 1\n",
      "236 1\n",
      "338 1\n",
      "177 1\n",
      "252 1\n",
      "552 1\n",
      "57 1\n",
      "74 1\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_tfidf(df):\n",
    "     \n",
    "    data_grouped = dict()\n",
    "    for idx,row in df.iterrows():\n",
    "        label = row['label']\n",
    "        category = label.split('_')[1]\n",
    "        year = label.split('_')[0]\n",
    "\n",
    "        if category in data_grouped:\n",
    "            docs = data_grouped[category]\n",
    "        else:\n",
    "            docs = list()\n",
    "\n",
    "        tokens = row['tokens']\n",
    "        docs_tmp = [\" \".join(x) for x in tokens]\n",
    "        docs = docs + docs_tmp\n",
    "        data_grouped[category] = docs\n",
    "    \n",
    "    tfidf_handlers = dict()\n",
    "    for k,v in data_grouped.items():\n",
    "        print(k, len(v))\n",
    "        cv = CountVectorizer(max_features=10000)\n",
    "        word_count_vector=cv.fit_transform(v)\n",
    "        feature_names=cv.get_feature_names()\n",
    "        tfidf_transformer=TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "        tfidf_handlers[k] = {'cv': cv, 'feature_names': feature_names, 'tfidf_transformer':tfidf_transformer}\n",
    "\n",
    "    tfidf_scores = {} #keep track of tf-idf scores for category by year\n",
    "    for idx,row in df.iterrows():\n",
    "        label = row['label']\n",
    "        category = label.split('_')[1]\n",
    "        year = label.split('_')[0]\n",
    "        \n",
    "        if category in tfidf_scores:\n",
    "            year_scores = tfidf_scores[category]\n",
    "        else:\n",
    "            year_scores = dict()\n",
    "\n",
    "        tfidf_transformer=tfidf_handlers[category]['tfidf_transformer']\n",
    "        feature_names=tfidf_handlers[category]['feature_names']\n",
    "        cv=tfidf_handlers[category]['cv']\n",
    "        tokens = row['tokens']\n",
    "        doc = [\" \".join(x) for x in tokens]\n",
    "\n",
    "        tf_idf_vector=tfidf_transformer.transform(cv.transform(doc))\n",
    "        sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "        keywords=extract_topn_from_vector(feature_names,sorted_items,100)\n",
    "        \n",
    "        year_scores[year] = keywords\n",
    "        tfidf_scores[category] = year_scores\n",
    "\n",
    "    return tfidf_scores\n",
    "\n",
    "tfidf_scores = prepare_data_for_tfidf(df_year_product_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 2000 192 5 2.604166666666667\n",
      "2000 2001 187 8 4.27807486631016\n",
      "2001 2002 191 2 1.0471204188481675\n",
      "2002 2003 187 7 3.7433155080213902\n",
      "2003 2004 186 5 2.6881720430107525\n",
      "2004 2005 181 5 2.7624309392265194\n",
      "2005 2006 176 5 2.840909090909091\n",
      "2006 2007 176 3 1.7045454545454544\n",
      "2007 2008 181 7 3.867403314917127\n",
      "2008 2009 186 5 2.6881720430107525\n",
      "2009 2010 175 11 6.2857142857142865\n",
      "2010 2011 181 7 3.867403314917127\n",
      "2011 2012 186 6 3.225806451612903\n",
      "2012 2013 186 6 3.225806451612903\n",
      "2013 2014 168 8 4.761904761904762\n",
      "2014 2015 143 12 8.391608391608392\n"
     ]
    }
   ],
   "source": [
    "def calc_temporal_overlap_tfidf(dic_of_scores):\n",
    "    \n",
    "    for k,v in dic_of_scores.items():\n",
    "        \n",
    "        if k == 'Software':\n",
    "            \n",
    "            ordered = OrderedDict(v)\n",
    "            keys = list(ordered.keys())\n",
    "            for i in range(0, len(keys)):\n",
    "                if i < len(keys)-1:\n",
    "                    year_n = keys[i]\n",
    "                    year_n1 = keys[i+1]\n",
    "                    total_terms = set(ordered[year_n].keys()).union(set(ordered[year_n1].keys()))\n",
    "                    overlap = set(ordered[year_n].keys()).intersection(set(ordered[year_n1].keys()))\n",
    "#                     print(total_terms)\n",
    "                    pct_overlap = len(overlap) / len(total_terms)*100\n",
    "                    print(year_n, year_n1, len(total_terms),len(overlap),pct_overlap)\n",
    "\n",
    "                    \n",
    "            \n",
    "calc_temporal_overlap_tfidf(tfidf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Here we're going to start to explore different modelling approaches to building our insights and predictive capability for this dataset.\n",
    "\n",
    "There's a number of model's we're going to first try to develop a word embedding model which will help us develop the underlying structure of our reviews/words which can then be used as latent representations for our sentiment scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "Before we can use our data, we need to convert it from Parquet to RecordIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def read_parquet_file(filename):\n",
    "    pf = ParquetFile(filename)\n",
    "    return pf.to_pandas()\n",
    "\n",
    "def convert_parquet_to_recordio(configs, manifest):\n",
    "    dfs = []\n",
    "     ##Create Augmented JSON Record file\n",
    "    aug_train = []\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    cnt = 0\n",
    "    maxi = 1000000\n",
    "    known_dates = {}\n",
    "    tot_rows = 0\n",
    "    for x in manifest:\n",
    "        if cnt < maxi:\n",
    "            date = x['date']\n",
    "            if date not in known_dates:\n",
    "                known_dates[date] = True\n",
    "                absolute_path = 's3://{}/{}'.format(configs['bucket_name'], x['path_with_prefix'])\n",
    "                folders = x['path_with_prefix'].split('/')\n",
    "                path = ''\n",
    "                for folder in folders:\n",
    "                    if 'part' not in folder:\n",
    "                        path = path + '/' + folder\n",
    "                absolute_path = 's3://{}{}'.format(configs['bucket_name'], path)\n",
    "                try:\n",
    "                    dataset = pq.ParquetDataset(absolute_path,filesystem=fs)\n",
    "                    table = dataset.read()\n",
    "#                     df = table.to_pandas()\n",
    "                    tot_rows += df.shape[0]\n",
    "                    print('date {}. rows {}'.format(x['date'],df.shape[0]))\n",
    "#                     dfs.append(df)\n",
    "                    cnt += 1\n",
    "\n",
    "                except:\n",
    "                    print('could not load files in folder {}'.format(x['date']))\n",
    "                \n",
    "    print('Total Rows {}'.format(tot_rows))\n",
    "    \n",
    "    \n",
    "#     train_file_s3 = 'train_lst/train_manifest.json'\n",
    "#     s3_bucket.upload_file('./train_manifest.json', train_file_s3)\n",
    "\n",
    "convert_parquet_to_recordio(configs, sampled_manifest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_manifest(configs, manifest):\n",
    "    \n",
    "     ##Create Augmented JSON Record file\n",
    "    aug_train = []\n",
    "    \n",
    "    for x in manifest:\n",
    "        absolute_path = 's3://{}/{}'.format(configs['bucket_name'], x['path_with_prefix'])\n",
    "        dic = {'source-ref':absolute_path}\n",
    "        aug_train.append(dic)\n",
    "        print(aug_train)\n",
    "        break\n",
    "    with open('train_manifest.json', 'w') as fout:\n",
    "        for x in aug_train:\n",
    "            fout.write(json.dumps(x)+'\\n')\n",
    "            #json.dump(aug_train, fout, indent=4)\n",
    "    \n",
    "        \n",
    "    \n",
    "#     train_file_s3 = 'train_lst/train_manifest.json'\n",
    "#     s3_bucket.upload_file('./train_manifest.json', train_file_s3)\n",
    "\n",
    "upload_manifest(configs, sampled_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
