{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews - Scaling the Chosen Methodology / Technique\n",
    "\n",
    "After the initial exploration and analysis stage of the data science pipeline, the next step will be to start to scale the data and methods being used to determine whether the results reflect that of the initial sample experiments.\n",
    "\n",
    "If we're driven by a particular scientifit enquiry rather than an application driven data science workflow, We should also be triyng to prove our original hypothesis. If we take the original work performed during the experimentation, we would look at our initial sample experiment and determine the confidence of our chosen model for predicting a specific label (let's say  we're predicting the product_cateogy). We would then try and establish whether the application of this method prooves our H0. \n",
    "\n",
    "While this is not strictly hypothesis testing (proving/disproving Null Hypothesis), in many scenarios, this tends to be a suitable approach to validate our assumptions when moving from sample to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following imports are required in order to run different statistical tests and modelling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Uncomment the folllowing lines on first run of the notebook. \n",
    "# !conda install -y -c conda-forge fastparquet scikit-learn arrow-cpp parquet-cpp pyarrow numpy\n",
    "# !pip install --upgrade mxnet gluonnlp swifter dask cufflinks\n",
    "# !pip install -q torch==1.4.0\n",
    "# !pip install -q transformers\n",
    "# !pip install s3-concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from s3_concat import S3Concat\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from fastparquet import write\n",
    "from fastparquet import ParquetFile\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "import glob\n",
    "import ast \n",
    "import csv\n",
    "import itertools\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "import multiprocessing\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs and Global Vars\n",
    "\n",
    "Throughout the notebook we're going to store all our global variables (although all variables inside a notebook are global if they are not defined in a method), inside an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'aws_region' :  'us-east-1',\n",
    "    'bucket_name': 'demos-amazon-reviews',\n",
    "    'prefix' : 'preprocessed_reviews_csvs', #only use this if you want to have your files in a folder \n",
    "    'index_key' : 'review_date_str',\n",
    "    'file_extension' :'.csv',\n",
    "    'wordvecdata': 'wordvec-full-data',\n",
    "    'models_dir': 'models',\n",
    "}\n",
    "\n",
    "global_vars = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Setting up the environment involves ensuring all the corret session and IAM roles are configured. We also need to ensure the correct region and bucket is made available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exists\n"
     ]
    }
   ],
   "source": [
    "def setup_env(configs, global_vars):\n",
    "    \n",
    "    sess = sagemaker.Session()\n",
    "    \n",
    "    role = get_execution_role()\n",
    "\n",
    "    AWS_REGION = configs['aws_region']\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    s3_bucket = s3.Bucket(configs['bucket_name'])\n",
    "\n",
    "    if s3_bucket.creation_date == None:\n",
    "    # create S3 bucket because it does not exist yet\n",
    "        print('Creating S3 bucket {}.'.format(bucket))\n",
    "        resp = s3.create_bucket(\n",
    "            ACL='private',\n",
    "            Bucket=bucket\n",
    "        )\n",
    "    else:\n",
    "        print('Bucket already exists')\n",
    "        \n",
    "    global_vars['role'] = role\n",
    "    global_vars['sess'] = sess\n",
    "    global_vars['s3'] = s3\n",
    "    global_vars['s3_bucket'] = s3_bucket\n",
    "    \n",
    "    return global_vars\n",
    "\n",
    "global_vars = setup_env(configs, global_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Manifest\n",
    "\n",
    "At this step, we need to create an index of all the files we're going to be using for this experiment and model building. Now, we don't want to download all of the data at once, or we're going to cause a lot of I/O activity for your Notebook Instance. \n",
    "\n",
    "What we're going to do is first create a path index to where the files live on S3. From there, we can do some sampling to get to see what the data looks like, do some basic sampling stats on the data, to get a better handle on how we should build a model, and then move to using all the data to build a robust model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 files\n",
      "Processed 200 files\n",
      "Training Dataset Size 241\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_manifest(configs, global_vars):\n",
    "    \n",
    "    interval_printer_idx = 100\n",
    "    idx = 0\n",
    "    1\n",
    "    conn = global_vars['s3_bucket']\n",
    "    file_format = configs['file_extension']\n",
    "    index_key = configs['index_key']+'='\n",
    "    s3_prefix = configs['prefix']+'/'\n",
    "    manifest = []    \n",
    "    for file in conn.objects.filter(Prefix=s3_prefix):\n",
    "        path = file.key\n",
    "#         print(file)\n",
    "        if (file_format in path):\n",
    "#             print(path)\n",
    "            relative_path = path.replace(configs['prefix'],'')\n",
    "            date = relative_path.split('/')[1].replace(index_key,'')\n",
    "\n",
    "            man = {'idx':idx, 'path':relative_path, 'path_with_prefix':path, 'date':date}\n",
    "            manifest.append(man)  \n",
    "            idx += 1\n",
    "            if (idx % interval_printer_idx) == 0:\n",
    "                print('Processed {} files'.format(idx))\n",
    "    print('Training Dataset Size {}'.format(len(manifest)))\n",
    "    return manifest\n",
    "            \n",
    "manifest = create_dataset_manifest(configs, global_vars)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5259983"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_lines(configs, global_vars, entry):\n",
    "        \n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    resp = s3.select_object_content(\n",
    "        Bucket=configs['bucket_name'],\n",
    "        Key=entry['path_with_prefix'],\n",
    "        ExpressionType='SQL',\n",
    "        Expression=\"SELECT count(*) FROM s3object s\",\n",
    "        InputSerialization = {'CSV':\n",
    "                              {\"FileHeaderInfo\": \"Use\", \n",
    "                               \"AllowQuotedRecordDelimiter\": True,\n",
    "                               \"QuoteEscapeCharacter\":\"\\\\\",\n",
    "                              }, \n",
    "                              'CompressionType': 'NONE'},\n",
    "        OutputSerialization = {'CSV':{}},\n",
    "    )\n",
    "    \n",
    "    for event in resp['Payload']:\n",
    "        if 'Records' in event:\n",
    "            records = event['Records']['Payload'].decode('utf-8')\n",
    "#             print('Rows:',records)\n",
    "            return(int(records))\n",
    "\n",
    "#sanity check that we have the right amount of data for a given file!\n",
    "count_lines(configs, global_vars, manifest[240])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    \n",
    "    df_len = df.shape[0]\n",
    "    pct_min = 0.01\n",
    "    min_product_category_row_count = df_len * pct_min #should be around 1% of the dataset, Imbalanced data will skew our modelling\n",
    "    df = df.groupby('product_category').filter(lambda x : len(x)>min_product_category_row_count)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prep_data_for_supervised_blazing_text_csv(df, train_file_output_name, test_file_output_name):\n",
    "    \n",
    "    \n",
    "    label_prefix = \"__label__\"    \n",
    "    labels = (label_prefix + df['product_category']).tolist()\n",
    "    #and tokenized words\n",
    "    tmp = df['review_body_processed']\n",
    "    xs = []\n",
    "    for entry in tmp:\n",
    "        res = str(entry).strip('][').split(', ') \n",
    "        res = ' '.join(res)\n",
    "        xs.append(res)\n",
    "    \n",
    "   \n",
    "    #split the data into test and train for supervised mode\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        xs, labels, random_state = 0)\n",
    "    \n",
    "    \n",
    "    train_prepped = []\n",
    "    #train\n",
    "    for i in range(0, len(X_train)):\n",
    "        \n",
    "        row = str(y_train[i]) + \" \" + str(X_train[i])\n",
    "        train_prepped.append([row])\n",
    "#     print('Train Processed Data: {}'.format(train_prepped[0]))\n",
    "    \n",
    "    test_prepped = []\n",
    "    #train\n",
    "    for i in range(0, len(X_test)):\n",
    "        row = str(y_test[i]) + \" \" + str(X_test[i])\n",
    "        test_prepped.append([row])\n",
    "    print('')\n",
    "#     print('Test Processed Data: {}'.format(test_prepped[0]))\n",
    "    \n",
    "    with open(train_file_output_name, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', \n",
    "                                lineterminator='\\n',  \n",
    "                                escapechar=' ', \n",
    "                                quoting=csv.QUOTE_NONE)\n",
    "        csv_writer.writerows(train_prepped)\n",
    "\n",
    "    with open(test_file_output_name, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', \n",
    "                                lineterminator='\\n',  \n",
    "                                escapechar=' ', \n",
    "                                quoting=csv.QUOTE_NONE)        \n",
    "        csv_writer.writerows(test_prepped)\n",
    "        \n",
    "    return True\n",
    "\n",
    "def prep_data_for_supervised_blazing_text_augmented(df, train_file_output_name, test_file_output_name):\n",
    "    \n",
    "    \n",
    "    label_prefix = \"__label__\"    \n",
    "    labels = df['product_category'].tolist()\n",
    "    #and tokenized words\n",
    "    tmp = df['review_body_processed']\n",
    "    xs = []\n",
    "    for entry in tmp:\n",
    "        res = str(entry).strip('][').split(', ') \n",
    "        res = ' '.join(res)\n",
    "        xs.append(res)\n",
    "        \n",
    "    #split the data into test and train for supervised mode\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        xs, labels, random_state = 0)\n",
    "    \n",
    "    \n",
    "    train_prepped = []\n",
    "    #train\n",
    "    for i in range(0, len(X_train)):\n",
    "        src = str(X_train[i])\n",
    "        if len(src)>10:\n",
    "            row = {'source':src,'label':str(y_train[i])}\n",
    "            train_prepped.append(row)\n",
    "#     print('Train Processed Data: {}'.format(train_prepped[0]))\n",
    "    \n",
    "    test_prepped = []\n",
    "    #train\n",
    "    for i in range(0, len(X_test)):\n",
    "        src = str(X_test[i])\n",
    "        if len(src)>10:\n",
    "            row = {'source':src,'label':str(y_test[i])}\n",
    "    #         row = str(y_test[i]) + \" \" + str(X_test[i])\n",
    "            test_prepped.append(row)\n",
    "    print('')\n",
    "#     print('Test Processed Data: {}'.format(test_prepped[0]))\n",
    "    \n",
    "    with open(train_file_output_name, 'w') as outfile:\n",
    "        for row in train_prepped:\n",
    "            outfile.write(json.dumps(row)+'\\n')\n",
    "#         json.dump(train_prepped, outfile, indent=\"\")\n",
    "\n",
    "    with open(test_file_output_name, 'w') as outfile:\n",
    "        for row in test_prepped:\n",
    "            outfile.write(json.dumps(row)+'\\n')\n",
    "    return True\n",
    "        \n",
    "        \n",
    "def upload_corpus_to_s3(configs, global_vars, train_file , test_file):\n",
    "    \n",
    "    \n",
    "    train_prefix = 'train'\n",
    "    test_prefix = 'test'\n",
    "    s3_bucket = global_vars['s3_bucket']\n",
    "    \n",
    "    sess = global_vars['sess']\n",
    "    bucket = global_vars['s3_bucket']\n",
    "   \n",
    "    data_file_s3 = '{}/{}/{}'.format(configs['wordvecdata'], train_prefix, train_file)\n",
    "    s3_bucket.upload_file(train_file, data_file_s3)   \n",
    "\n",
    "    data_file_s3 = '{}/{}/{}'.format(configs['wordvecdata'], test_prefix, test_file)\n",
    "    s3_bucket.upload_file(test_file, data_file_s3) \n",
    "    \n",
    "    s3_train_data = 's3://{}/{}/{}'.format(configs['bucket_name'], configs['wordvecdata'], train_prefix)\n",
    "    s3_test_data = 's3://{}/{}/{}'.format(configs['bucket_name'], configs['wordvecdata'], test_prefix)\n",
    "    s3_output_location = 's3://{}/{}/output'.format(configs['bucket_name'], configs['wordvecdata'])\n",
    "    \n",
    "    configs['s3_w2v_train_data'] = s3_train_data\n",
    "    configs['s3_w2v_test_data'] = s3_test_data\n",
    "    configs['s3_w2v_output_location'] = s3_output_location\n",
    "\n",
    "    print('S3 Training Data Path {}'.format(s3_train_data))\n",
    "    print('S3 Test Data Path {}'.format(s3_test_data))\n",
    "    print('S3 output Data Path {}'.format(s3_output_location))\n",
    "\n",
    "    return configs\n",
    "\n",
    "def remove_local_file(filename):\n",
    "    \n",
    "    os.remove(filename)\n",
    "    \n",
    "def download_transform_upload(configs, global_vars, manifest):\n",
    "        \n",
    "    #As we're dealing with a large dataset, we need to be strategic \n",
    "    \n",
    "    partNum = 0\n",
    "    for entry in manifest:\n",
    "        full_path = 's3://'+configs['bucket_name']+'/'+entry['path_with_prefix']\n",
    "        df = pd.read_csv(full_path, header=0, error_bad_lines=False, escapechar=\"\\\\\")\n",
    "        print('Dataset Rows {}, Columns {}'.format(df.shape[0], df.shape[1]))\n",
    "        df = prep_data(df)\n",
    "        \n",
    "        train_file = 'amazonreviews_part_{}.train'.format(partNum)\n",
    "        test_file = 'amazonreviews_part_{}.test'.format(partNum)\n",
    "        \n",
    "        if prep_data_for_supervised_blazing_text_augmented(df, train_file, test_file):\n",
    "            #upload new train file\n",
    "            configs = upload_corpus_to_s3(configs, global_vars, train_file , test_file)         \n",
    "            #delete local file\n",
    "            remove_local_file(train_file)\n",
    "            remove_local_file(test_file)\n",
    "            partNum += 1\n",
    "        else:\n",
    "            print('Could not process File {}'.format(full_path))\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 23, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 19, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 27, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 45, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 46, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 39, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 69, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 30, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 348, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 434, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 415, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 468, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 586, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 541, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 657, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 1118, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 1567, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 1510, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 2056, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 2590, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 1655, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 3273, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 3498, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 3472, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 2650, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 4682, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 4555, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1434: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 5214, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 6766, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 5199, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 7537, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 8277, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 8950, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 14705, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 18131, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 16007, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 14754, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 18154, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 20523, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 23464, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 25706, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 26085, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 29021, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 30794, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 29571, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 34012, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 34737, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 33653, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 31272, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2909: DtypeWarning: Columns (14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 35460, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 44151, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 79807, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 83417, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 72891, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 91563, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 94683, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 91532, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 104234, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3060: expected 18 fields, saw 75\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 90017, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 79224: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 81109, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 65477, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2909: DtypeWarning: Columns (8,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 71941, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 10724: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 46605: expected 18 fields, saw 37\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 64475, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 66688, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 70538, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 62081, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 67995, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 63441, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 32153: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 72207, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 4994: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 77716, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 82284, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 81643, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 63106, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 71604, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 81122, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 81509, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 84779, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 19642: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 70447, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 79025, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 75563, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 71320, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 70862, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 77625, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8011: expected 18 fields, saw 26\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 71710, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 66602, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 78862, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 77524, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 88210, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 92240, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 45262: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 75373, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 7114: expected 18 fields, saw 26\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 81213, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 78506, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 76810, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 86750, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 85934, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 78674, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 78979, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 90497, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 92013, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 79951: expected 18 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 96123, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 4799: expected 18 fields, saw 30\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 101887, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 91068, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 93147, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 92508, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 88535, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 85199: expected 18 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 94735, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 91228, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 76805, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 74265, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 78701, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16937: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 67400: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 80508, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 88194, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 95781, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 47572: expected 18 fields, saw 28\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 78793, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 44777: expected 18 fields, saw 26\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 90743, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 83098, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 29367: expected 18 fields, saw 26\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 84577, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 51287: expected 18 fields, saw 26\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 85711, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5216: expected 18 fields, saw 29\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 103145, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 132469, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 22930: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 132218, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 292: expected 18 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 118777, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 96786, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 106540, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 119913, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 124285, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 79321: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 152398, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 56503, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 95385, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 103895, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 71876: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 123681, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 122991, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 75515, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 77120, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 133674, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 97390, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 290924, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 29877: expected 18 fields, saw 25\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 137420, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 175268, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 115010, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 184708, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 137037, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 154010, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 157734, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 154620, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 150471: expected 18 fields, saw 31\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 154999, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 154494, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 193485, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 227001, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 109600: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 183445, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 127580: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 173230, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 170299: expected 18 fields, saw 33\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 181281, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 129424: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 178596, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 180954, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 189676, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6303: expected 18 fields, saw 50\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 187649, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 190914, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 191232, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 127305: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 186665, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 231229, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 300814, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 229109, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 228743, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 160003: expected 18 fields, saw 46\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 226426, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 101492: expected 18 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 232363, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 232692, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 202024, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 220256, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 246444: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 265092, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 224730: expected 18 fields, saw 34\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 247477, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 247074, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 295955, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 315542, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 165406: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 330471, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 296907, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 259826: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 264590, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 444: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 265664: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 268183, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 263428, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 308800, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 53053: expected 18 fields, saw 25\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 313183, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 258037: expected 18 fields, saw 49\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 338142, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 62844: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 339632, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 355057, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 365118: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 435037, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 116476: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 183741: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 243671: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 515924, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 78688: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 104343: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 194984: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 255976: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 283054: expected 18 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 407372, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6946: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 345065: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 412831: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 431015: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 448239, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 267725: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 429699: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 430575, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 429266, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 146896: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 452754, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43114: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 229635: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 478487, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 154403: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 452308: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 509161, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 53188: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 262789: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 388306: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 470556: expected 18 fields, saw 45\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 533598, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 544948: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 545043, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16264: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 172230: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 332015: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 371153: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 423263: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 485818: expected 18 fields, saw 44\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 568545, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 179135: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 212258: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 646708: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 655386: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 743941, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 105653: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 849719: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 859212, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 666460: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 671965, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 12362: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 486586: expected 18 fields, saw 35\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 725497, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 250802: expected 18 fields, saw 28\\n'\n",
      "b'Skipping line 372473: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 427570: expected 18 fields, saw 26\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 670426, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n",
      "Dataset Rows 685392, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 166263: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 643436: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 684144: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 699200, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 207449: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 439715: expected 18 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 757924, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 10196: expected 18 fields, saw 40\\n'\n",
      "b'Skipping line 179501: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 772142, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 169739: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 325163: expected 18 fields, saw 25\\n'\n",
      "b'Skipping line 426128: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 473867: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 830764: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 901447: expected 18 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 915242, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 276669: expected 18 fields, saw 20\\nSkipping line 288877: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 295408: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 846905: expected 18 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 1025940, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 40451: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 519374: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 774344: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 799837: expected 18 fields, saw 44\\n'\n",
      "b'Skipping line 1195376: expected 18 fields, saw 30\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 1304875, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1323002: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1504371: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 2172111: expected 18 fields, saw 24\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2388762, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 107662: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 182450: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 242822: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 553668: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1107339: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1394760: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1564840: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1800362: expected 18 fields, saw 25\\n'\n",
      "b'Skipping line 1942145: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2826699, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 547502: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 641957: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1181218: expected 18 fields, saw 32\\n'\n",
      "b'Skipping line 1368433: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1715896: expected 18 fields, saw 39\\n'\n",
      "b'Skipping line 1950271: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2112904: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2227541, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 339715: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 374722: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 909974: expected 18 fields, saw 25\\n'\n",
      "b'Skipping line 1255391: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1470915: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1849468: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 2003151: expected 18 fields, saw 30\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2329497, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 595037: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1106871: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 1467716: expected 18 fields, saw 22\\nSkipping line 1470303: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1648052: expected 18 fields, saw 27\\n'\n",
      "b'Skipping line 1688088: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 507022: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 603880: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 665048: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 997271: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1191284: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1544984: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 1578878: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2303639, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 346786: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 376777: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 437764: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 601927: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 1017710: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 1350411: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1723666: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1943655: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2147647, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 392761: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 597522: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 790320: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1102921: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1368570: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1461203: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 1729183: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1981788: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2113686: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2900715: expected 18 fields, saw 33\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2905588, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 11196: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 374306: expected 18 fields, saw 21\\nSkipping line 385562: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 1237776: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2111518: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 2385917: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3008194: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 3095815: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3195837: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 3587244, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 31529: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 37231: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 235882: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 314897: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 544557: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1017556: expected 18 fields, saw 36\\n'\n",
      "b'Skipping line 1084260: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2829855, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 243150: expected 18 fields, saw 32\\n'\n",
      "b'Skipping line 1468142: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 1867001: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1953176: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 3022435, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 272799: expected 18 fields, saw 25\\n'\n",
      "b'Skipping line 344949: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 460948: expected 18 fields, saw 19\\nSkipping line 474877: expected 18 fields, saw 28\\n'\n",
      "b'Skipping line 1078278: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1493872: expected 18 fields, saw 31\\n'\n",
      "b'Skipping line 1599672: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2284529: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 2592035: expected 18 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2682400, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3530: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 406300: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 469217: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1188257: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2638420, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 839478: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1072193: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1470670: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2020565: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 2113841: expected 18 fields, saw 19\\nSkipping line 2115355: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 2708013, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 453892: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 973358: expected 18 fields, saw 27\\n'\n",
      "b'Skipping line 1318559: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 1393960: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1856130: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2109455: expected 18 fields, saw 28\\n'\n",
      "b'Skipping line 2248178: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 2390685: expected 18 fields, saw 20\\nSkipping line 2390691: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 2799889: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2980239: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 3301518: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 3845988: expected 18 fields, saw 21\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 4064186, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6397: expected 18 fields, saw 25\\nSkipping line 10732: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 100070: expected 18 fields, saw 36\\n'\n",
      "b'Skipping line 490471: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 533342: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 716501: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1090740: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1281270: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1789827: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1822817: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2099836: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2421976: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 2861821: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 3687915: expected 18 fields, saw 27\\n'\n",
      "b'Skipping line 4023985: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 4146029, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1309528: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1501414: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2159454: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 2625075: expected 18 fields, saw 19\\nSkipping line 2628095: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 3017005: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 3956179, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 129716: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 335169: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 754967: expected 18 fields, saw 25\\n'\n",
      "b'Skipping line 1321485: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 1715179: expected 18 fields, saw 21\\nSkipping line 1722715: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 3044482: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 3381768: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 3409741: expected 18 fields, saw 28\\nSkipping line 3434558: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 3941093: expected 18 fields, saw 29\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 4224033, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 213964: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1057558: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1435053: expected 18 fields, saw 30\\n'\n",
      "b'Skipping line 3199147: expected 18 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 4139016, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3019488: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 3798245: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 4444169: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 4581898: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 4728182: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 5203522: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 5270395: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 5326682: expected 18 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 5371561, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 551412: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 790934: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1195394: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 1443641: expected 18 fields, saw 29\\n'\n",
      "b'Skipping line 3011004: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3825037: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 4594412: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 4940090: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 5249010: expected 18 fields, saw 25\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 5498159, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 380807: expected 18 fields, saw 28\\n'\n",
      "b'Skipping line 560494: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 697659: expected 18 fields, saw 33\\n'\n",
      "b'Skipping line 1273621: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1444690: expected 18 fields, saw 26\\n'\n",
      "b'Skipping line 1874542: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2630979: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3307929: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 3498867: expected 18 fields, saw 28\\n'\n",
      "b'Skipping line 3668380: expected 18 fields, saw 22\\n'\n",
      "b'Skipping line 3854370: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 3981179: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 4643530: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 5076247, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 163084: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 226951: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 1296093: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 2527761: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3913350: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 5529430, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 4361: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 403825: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 1750832: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2150291: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2833078: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 3127176: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 4170957: expected 18 fields, saw 19\\nSkipping line 4181051: expected 18 fields, saw 24\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 4827214, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 10707: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 483511: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 568213: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1199638: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1606838: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 1909690: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2009220: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 2045041: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 3004554: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3759389: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 4318231: expected 18 fields, saw 32\\n'\n",
      "b'Skipping line 4479895: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 4739672, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 791124: expected 18 fields, saw 19\\nSkipping line 815704: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 930360: expected 18 fields, saw 25\\n'\n",
      "b'Skipping line 1896268: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2138116: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 2361543: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 3252623: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3773000: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 4018364: expected 18 fields, saw 24\\n'\n",
      "b'Skipping line 4673395: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 4706059: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 4774088, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 782600: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 2904351: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 3172892: expected 18 fields, saw 42\\n'\n",
      "b'Skipping line 3224820: expected 18 fields, saw 51\\n'\n",
      "b'Skipping line 3501485: expected 18 fields, saw 21\\n'\n",
      "b'Skipping line 3528793: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 4126729: expected 18 fields, saw 19\\n'\n",
      "b'Skipping line 4201875: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 4788045: expected 18 fields, saw 20\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 5144760, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2354843: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 4076974: expected 18 fields, saw 20\\n'\n",
      "b'Skipping line 4685772: expected 18 fields, saw 23\\n'\n",
      "b'Skipping line 4883245: expected 18 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Rows 5259979, Columns 18\n",
      "\n",
      "S3 Training Data Path s3://demos-amazon-reviews/wordvec-full-data/train\n",
      "S3 Test Data Path s3://demos-amazon-reviews/wordvec-full-data/test\n",
      "S3 output Data Path s3://demos-amazon-reviews/wordvec-full-data/output\n"
     ]
    }
   ],
   "source": [
    "download_transform_upload(configs, global_vars, manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordvec-full-data/train/\n",
      "wordvec-full-data/test/\n"
     ]
    }
   ],
   "source": [
    "#as blazingtext pipe only supports one augmented file for train and test, let's concat them all\n",
    "def concat_augmented_files(configs, global_vars):\n",
    "    \n",
    "    #output filename\n",
    "    concatenated_file_train = 'amazon_augmented_train.json'\n",
    "    concatenated_file_test = 'amazon_augmented_test.json'\n",
    "    \n",
    "    #where all our files sit\n",
    "    train_prefix = 'train'\n",
    "    test_prefix = 'test'\n",
    "    s3_train_path = '{}/{}/'.format(configs['wordvecdata'], train_prefix)\n",
    "    s3_test_path = '{}/{}/'.format(configs['wordvecdata'], test_prefix)\n",
    "    \n",
    "    \n",
    "    s3_concat_file_path_train = '{}/{}'.format(configs['s3_w2v_train_data'], concatenated_file_train)\n",
    "    s3_concat_file_path_test = '{}/{}'.format(configs['s3_w2v_test_data'], concatenated_file_test)\n",
    "    \n",
    "    \n",
    "    print(s3_train_path)\n",
    "    print(s3_test_path)\n",
    "\n",
    "    min_file_size = None\n",
    "\n",
    "    #train file\n",
    "    job_train = S3Concat(configs['bucket_name'], \n",
    "                         s3_concat_file_path_train, \n",
    "                         min_file_size,\n",
    "                         content_type='application/json',\n",
    "                         session=boto3.session.Session()\n",
    "                        )\n",
    "    \n",
    "    job_train.add_files(s3_train_path)\n",
    "    job_train.concat(small_parts_threads=32)\n",
    "\n",
    "    \n",
    "    #test file\n",
    "    job_test = S3Concat(configs['bucket_name'], \n",
    "                         s3_concat_file_path_test, \n",
    "                         min_file_size,\n",
    "                         content_type='application/json',\n",
    "                         session=boto3.session.Session()\n",
    "                        )\n",
    "    \n",
    "    job_test.add_files(s3_test_path)\n",
    "    job_test.concat(small_parts_threads=32)\n",
    "    \n",
    "    \n",
    "    configs['s3_w2v_train_file'] = s3_concat_file_path_train\n",
    "    \n",
    "    configs['s3_w2v_test_file'] = s3_concat_file_path_test\n",
    "    \n",
    "    return configs\n",
    "\n",
    "configs = concat_augmented_files(configs, global_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1000000 lines\n",
      "read 2000000 lines\n",
      "read 3000000 lines\n",
      "read 4000000 lines\n",
      "read 5000000 lines\n",
      "read 6000000 lines\n",
      "read 7000000 lines\n",
      "read 8000000 lines\n",
      "read 9000000 lines\n",
      "read 10000000 lines\n",
      "read 11000000 lines\n",
      "read 12000000 lines\n",
      "read 13000000 lines\n",
      "read 14000000 lines\n",
      "read 15000000 lines\n",
      "read 16000000 lines\n",
      "read 17000000 lines\n",
      "read 18000000 lines\n",
      "read 19000000 lines\n",
      "read 20000000 lines\n",
      "read 21000000 lines\n",
      "read 22000000 lines\n",
      "read 23000000 lines\n",
      "read 24000000 lines\n",
      "read 25000000 lines\n",
      "read 26000000 lines\n",
      "read 27000000 lines\n",
      "read 28000000 lines\n",
      "read 29000000 lines\n",
      "read 30000000 lines\n",
      "read 31000000 lines\n",
      "read 32000000 lines\n",
      "read 33000000 lines\n",
      "read 34000000 lines\n",
      "read 35000000 lines\n",
      "read 36000000 lines\n",
      "read 37000000 lines\n",
      "read 38000000 lines\n",
      "read 39000000 lines\n",
      "read 40000000 lines\n",
      "read 41000000 lines\n",
      "read 42000000 lines\n",
      "read 43000000 lines\n",
      "read 44000000 lines\n",
      "read 45000000 lines\n",
      "read 46000000 lines\n",
      "read 47000000 lines\n",
      "read 48000000 lines\n",
      "read 49000000 lines\n",
      "read 50000000 lines\n",
      "read 51000000 lines\n",
      "read 52000000 lines\n",
      "read 53000000 lines\n",
      "read 54000000 lines\n",
      "read 55000000 lines\n",
      "read 56000000 lines\n",
      "read 57000000 lines\n",
      "read 58000000 lines\n",
      "read 59000000 lines\n",
      "read 60000000 lines\n",
      "read 61000000 lines\n",
      "read 62000000 lines\n",
      "read 63000000 lines\n",
      "read 64000000 lines\n",
      "read 65000000 lines\n",
      "read 66000000 lines\n",
      "read 67000000 lines\n",
      "read 68000000 lines\n",
      "read 69000000 lines\n",
      "read 70000000 lines\n",
      "read 71000000 lines\n",
      "read 72000000 lines\n",
      "read 73000000 lines\n",
      "read 74000000 lines\n",
      "read 75000000 lines\n",
      "read 76000000 lines\n",
      "read 77000000 lines\n",
      "read 78000000 lines\n",
      "read 79000000 lines\n",
      "read 80000000 lines\n",
      "read 81000000 lines\n",
      "read 82000000 lines\n",
      "read 83000000 lines\n",
      "read 84000000 lines\n",
      "read 85000000 lines\n",
      "read 86000000 lines\n",
      "read 87000000 lines\n",
      "read 88000000 lines\n",
      "read 89000000 lines\n",
      "read 90000000 lines\n",
      "read 91000000 lines\n",
      "read 92000000 lines\n",
      "read 93000000 lines\n",
      "read 94000000 lines\n",
      "read 95000000 lines\n",
      "read 96000000 lines\n",
      "read 97000000 lines\n",
      "read 98000000 lines\n",
      "read 99000000 lines\n",
      "read 100000000 lines\n",
      "read 101000000 lines\n",
      "read 102000000 lines\n",
      "read 103000000 lines\n",
      "read 104000000 lines\n",
      "read 105000000 lines\n",
      "read 106000000 lines\n",
      "Total parsed lines 106410466\n"
     ]
    }
   ],
   "source": [
    "### Validate Data\n",
    "def validate_jsonlines(filename):\n",
    "    \n",
    "    lines_cnt = 0\n",
    "    with open(filename, 'r') as jfile:\n",
    "        for line in jfile:\n",
    "            try:\n",
    "                line_loaded = json.loads(line)\n",
    "#                 print(line_loaded)\n",
    "                lines_cnt += 1\n",
    "#                 break\n",
    "                if (lines_cnt % 1000000) == 0:\n",
    "                    print('read {} lines'.format(lines_cnt))\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('error in line {}'.format(line))\n",
    "                \n",
    "    print('Total parsed lines {}'.format(lines_cnt))\n",
    "    \n",
    "validate_jsonlines('amazon_augmented_train.json')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model /Analysis Experimentation (Local Mode)\n",
    "\n",
    "The purpose of this section is to perform some experimentations with different modelling techniques.\n",
    "\n",
    "We're first going to perform some local experiments on the 1% sample of data to see which methods provide valuable insights for both customers (e.g. Amazon Customer), and operations (e.g. Amazon). \n",
    "\n",
    "We want to look at different type of insights, from understanding how customer reviews have changed over times, and whether there is predictability in the type of review, and the category of product it is related to. \n",
    "\n",
    "Let's start of by first gettign our data into a shape which we can use for analysis and modelling purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data for Modelling Purposes\n",
    "\n",
    "We're going to develop some dataframes which represent our Xs and Ys (features and labels).\n",
    "\n",
    "Let's create some feature/label datasets which are shaped around the following labels:\n",
    "\n",
    "- year_product-category\n",
    "- product-category_star_rating\n",
    "\n",
    "The features for this model will be only using the text of the reviews\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Using BlazingText (Supervised)\n",
    "BlazingText expects a single preprocessed text file with space separated tokens and each line of the file should contain a single sentence and the corresponding label(s) prefixed by \"_ _label_ _\".\n",
    "\n",
    "As we're now using the complete dataset, we'll need to use the Augmented dataset structure and use `Pipe` mode in  order to allow for streaming of data, rather than loading all the data into memory in one go.\n",
    "\n",
    "Augmented Data Structure\n",
    "\n",
    "```json\n",
    "{'source':'string', 'label':'string'}\n",
    "{'source':'string', 'label':'string'}\n",
    "```\n",
    "\n",
    "Note, the structure are single json entries, per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "def configure_estimator(configs, global_vars):\n",
    "    \n",
    "    region_name = configs['aws_region'] \n",
    "    sess = global_vars['sess']\n",
    "    container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "    print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))\n",
    "\n",
    "    bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         global_vars['role'], \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c5.18xlarge',\n",
    "                                         train_volume_size = 150,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'Pipe',\n",
    "                                         output_path=configs['s3_w2v_output_location'],\n",
    "                                         sagemaker_session=sess)\n",
    "    \n",
    "    bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                                 epochs=20,\n",
    "                                 min_count=2,\n",
    "                                 learning_rate=0.05,\n",
    "                                 vector_dim=10,\n",
    "                                 early_stopping=True,\n",
    "                                 patience=4,\n",
    "                                 min_epochs=10,\n",
    "                                 word_ngrams=4)\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    global_vars['bt_model'] = bt_model\n",
    "    \n",
    "    return global_vars\n",
    "\n",
    "global_vars = configure_estimator(configs, global_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_data_channels(configs, global_vars):\n",
    "    \n",
    "\n",
    "    s3train_manifest = configs['s3_w2v_train_file'] \n",
    "    s3validation_manifest = configs['s3_w2v_test_file'] \n",
    "    \n",
    "    attribute_names = [\"source\",\"label\"]\n",
    "\n",
    "    \n",
    "    train_data = sagemaker.session.s3_input(s3train_manifest, \n",
    "                                            distribution='FullyReplicated', \n",
    "                                            content_type='application/jsonlines', \n",
    "                                            s3_data_type='AugmentedManifestFile',\n",
    "                                            attribute_names=attribute_names,\n",
    "                                            record_wrapping='RecordIO' \n",
    "                                           )\n",
    "    \n",
    "    validation_data = sagemaker.session.s3_input(s3validation_manifest, \n",
    "                                                 distribution='FullyReplicated', \n",
    "                                                 content_type='application/jsonlines', \n",
    "                                                 s3_data_type='AugmentedManifestFile',\n",
    "                                                 attribute_names=attribute_names,\n",
    "                                                 record_wrapping='RecordIO'\n",
    "                                                )\n",
    "    \n",
    "    data_channels = {'train': train_data, 'validation': validation_data}\n",
    "    \n",
    "    global_vars['data_channels'] = data_channels\n",
    "\n",
    "    return global_vars\n",
    "\n",
    "global_vars = configure_data_channels(configs, global_vars)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-29 06:58:01 Starting - Starting the training job...\n",
      "2020-04-29 06:58:03 Starting - Launching requested ML instances......\n",
      "2020-04-29 06:59:11 Starting - Preparing the instances for training......\n",
      "2020-04-29 07:00:08 Downloading - Downloading input data......................................................................................................\n",
      "2020-04-29 07:17:39 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/29/2020 07:17:55 WARNING 139910984623936] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/29/2020 07:17:55 WARNING 139910984623936] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/29/2020 07:17:55 INFO 139910984623936] nvidia-smi took: 0.0252721309662 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/29/2020 07:17:55 INFO 139910984623936] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mterminate called after throwing an instance of 'std::runtime_error'\n",
      "  what():  Customer Error: Invalid JSON string found. Please ensure data is in the expected JSON formats\u001b[0m\n",
      "\n",
      "2020-04-29 07:18:11 Uploading - Uploading generated training model\n",
      "2020-04-29 07:18:11 Failed - Training job failed\n",
      "\u001b[34m[04/29/2020 07:18:00 ERROR 139910984623936] Customer Error: Training did not complete successfully! Please check the logs for errors.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/amazon/lib/python2.7/site-packages/blazingtext/train.py\", line 75, in main\n",
      "    train_blazing_single(resource_config, train_config, data_config)\n",
      "  File \"/opt/amazon/lib/python2.7/site-packages/blazingtext/train_methods.py\", line 245, in train_blazing_single\n",
      "    raise exceptions.CustomerError(\"Training did not complete successfully! Please check the logs for errors.\")\u001b[0m\n",
      "\u001b[34mCustomerError: Training did not complete successfully! Please check the logs for errors.\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job blazingtext-2020-04-29-06-58-01-582: Failed. Reason: ClientError: Training did not complete successfully! Please check the logs for errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-d1c92a7136da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-d1c92a7136da>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(configs, global_vars)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bt_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2613\u001b[0m                 ),\n\u001b[1;32m   2614\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2615\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2616\u001b[0m             )\n\u001b[1;32m   2617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job blazingtext-2020-04-29-06-58-01-582: Failed. Reason: ClientError: Training did not complete successfully! Please check the logs for errors."
     ]
    }
   ],
   "source": [
    "def fit_model(configs, global_vars):\n",
    "    \n",
    "    bt_model = global_vars['bt_model']\n",
    "    data_channels = global_vars['data_channels']\n",
    "    bt_model.fit(inputs=data_channels, logs=True)\n",
    "    \n",
    "    \n",
    "fit_model(configs, global_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def host_model(global_vars):\n",
    "    \n",
    "    bt_model = global_vars['bt_model']\n",
    "    text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n",
    "    global_vars['w2v_classifier'] = text_classifier\n",
    "    \n",
    "    return global_vars\n",
    "\n",
    "global_vars = host_model(global_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data_against_model(global_vars, test_file):\n",
    "    \n",
    "    train_data = []\n",
    "    instances = []\n",
    "    with open(test_file, 'r', newline= '\\n') as csvinputfile:\n",
    "        data = csv.reader(csvinputfile)\n",
    "        for row in data:\n",
    "#             print(row)\n",
    "            label = row[0].split(' ')[0]\n",
    "            text = row[0].partition(' ')[2].replace('  ',' ').strip()\n",
    "            tmp = {'label':label, \"text\":text}\n",
    "            train_data.append(tmp)\n",
    "#             print(tmp)\n",
    "#             break\n",
    "            #same order as the csv rows\n",
    "            instances.append(text)\n",
    "            \n",
    "    print('Total Instances {}. Total Train Data {}'.format(len(instances), len(train_data)))\n",
    "\n",
    "#     print(instances[0], train_data[0]['label'])\n",
    "    \n",
    "    # we need to do some batch inferencing due to the size of the data:\n",
    "    \n",
    "    #each batch is 1000 sentences\n",
    "    batch_size = 10000\n",
    "    batches = len(instances) // batch_size\n",
    "    \n",
    "    print('Batches {}'.format(batches))\n",
    "    \n",
    "    predictions_batches = []\n",
    "    \n",
    "    for i in range(0, batches+1):\n",
    "        lower = batch_size * i\n",
    "        upper = batch_size * (i+1)\n",
    "        if i == batches:\n",
    "            upper = len(instances)\n",
    "        print('Batch {} : {}'.format(lower,upper))\n",
    "            \n",
    "        instances_batch = instances[lower:upper]\n",
    "        \n",
    "        payload = {\"instances\":instances_batch,\n",
    "                  \"configuration\": {\"k\": 1}}\n",
    "\n",
    "        text_classifier =  global_vars['w2v_classifier']\n",
    "\n",
    "\n",
    "        response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "        predictions = json.loads(response)\n",
    "        predictions_batches.append(predictions)\n",
    "        \n",
    "#     print(json.dumps(predictions, indent=2))\n",
    "    print('Total Predictions {}'.format(len(predictions)))\n",
    "        \n",
    "    return predictions_batches, train_data\n",
    "            \n",
    "              \n",
    "predictions_batches, train_data = evaluate_test_data_against_model(global_vars, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(predictions_batches, train_data):\n",
    "    \n",
    "    preds = []\n",
    "    for batch in predictions_batches:\n",
    "        for pred in batch:\n",
    "            lab = pred['label'][0]\n",
    "            prob = pred['prob'][0]\n",
    "            tmp = {'pred_label':lab, 'pred_prob':prob}\n",
    "            preds.append(tmp)\n",
    "            \n",
    "    print('Total Preds {}'.format(len(preds)))\n",
    "    \n",
    "    for i in range(0,len(train_data)):\n",
    "        data = train_data[i]\n",
    "        true_label = data['label']\n",
    "        preds[i]['true_label'] = true_label\n",
    "        \n",
    "    print('Example Data: \\n\\t {}'.format(preds[1]))\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for pred in preds:\n",
    "        y_true.append(pred['true_label'].replace('__label__',''))\n",
    "        y_pred.append(pred['pred_label'].replace('__label__',''))\n",
    "        \n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "        \n",
    "evaluate_model_predictions(predictions_batches, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: Using the Word2Vec word Embedding approach, we're seeing similar results to the TF-IDF/SVC implementation for predicting product category. However, the computational time required to compute the SVC was nearly 100 times slower than the Word2Vec approach, and this is only for a sample dataset of 1% of the total data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/bert.html#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-20.0.2\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |██████████████████████          | 288.9 MB 111.7 MB/s eta 0:00:02     |██████████████▎                 | 187.6 MB 116.6 MB/s eta 0:00:03     |█████████████████████▌          | 282.9 MB 111.7 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[K     |███████████████████████████████▉| 420.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 420.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 420.9 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.0 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.1 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.2 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.3 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.4 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.5 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.6 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.7 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.8 MB 118.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 421.8 MB 118.2 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 421.8 MB 20 kB/s \n",
      "\u001b[?25hProcessing /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6/termcolor-1.1.0-cp36-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 90.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Processing /home/ec2-user/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a/wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 111.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 112.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.1 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow) (39.1.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.14.0-py2.py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 16.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.14.1)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 124.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.4.2)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 112.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=6587 sha256=e2fff6b0c34f897e238a4867501ea5689862b8049bba44354ea404151079ceb2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "Successfully built gast\n",
      "\u001b[31mERROR: google-auth 1.14.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.1.1 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: termcolor, six, pyasn1-modules, cachetools, google-auth, oauthlib, requests, requests-oauthlib, google-auth-oauthlib, markdown, grpcio, tensorboard, gast, wrapt, google-pasta, tensorflow-estimator, opt-einsum, tensorflow\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.20.0\n",
      "    Uninstalling requests-2.20.0:\n",
      "      Successfully uninstalled requests-2.20.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.10.11\n",
      "\u001b[31mERROR: Cannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, BertTokenizer, DistilBertModel, BertModel\n",
    "from multiprocessing import  Pool\n",
    "import multiprocessing as mp\n",
    "import torch.multiprocessing as torchmp\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig, \n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification\n",
    ")\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bert_configs(configs, batch_size, epochs):\n",
    "    configs['bert_model_type'] = 'bert-base-uncased'\n",
    "    configs['bert_epochs'] = 3\n",
    "    configs['bert_batch_size'] = 1024\n",
    "    configs['model_path'] = 'model/bert_tf_model_uncased_amazon_reviews_fine_tuned'\n",
    "    return configs\n",
    "\n",
    "configs = set_bert_configs(configs, batch_size = 1024, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Training Of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_493 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  23070     \n",
      "=================================================================\n",
      "Total params: 109,505,310\n",
      "Trainable params: 109,505,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def describe_model(configs):\n",
    "    \n",
    "    TO_FINETUNE = configs = configs['bert_model_type']\n",
    "    num_examples = len(tuples)\n",
    "    config = BertConfig.from_pretrained(TO_FINETUNE)\n",
    "    model = TFBertForSequenceClassification.from_pretrained(TO_FINETUNE, config=config) \n",
    "    model.summary()\n",
    "\n",
    "describe_model(configs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes 30\n"
     ]
    }
   ],
   "source": [
    "def create_label_mapping(df):\n",
    "    \n",
    "    num_labels = df['product_category'].nunique()\n",
    "    labels_unique = df['product_category'].unique()\n",
    "    print('Number of Classes {}'.format(num_labels))\n",
    "    \n",
    "    labels = df['product_category'].tolist()\n",
    "    #convert to index for reversals\n",
    "    labels_unique_name_idx = { labels_unique[i] :  i for i in range(0, len(labels_unique) ) }\n",
    "    labels_unique_idx_name = { i : labels_unique[i] for i in range(0, len(labels_unique) ) }\n",
    "#     print(labels_unique_name_idx)\n",
    "    \n",
    "    labels_idx_col = []\n",
    "    for lab in labels:\n",
    "        if lab in labels_unique_name_idx:\n",
    "            labels_idx_col.append(labels_unique_name_idx[lab])\n",
    "    \n",
    "    df['label'] = labels_idx_col\n",
    "    \n",
    "#     display(df)\n",
    "\n",
    "    return df, labels_unique_idx_name, num_labels\n",
    "\n",
    "sampled_data, labels_mapping, num_labels = create_label_mapping(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_column(df,text_col):\n",
    "    \n",
    "    col_to_add = 'processed_text'\n",
    "    tmp = df[text_col]\n",
    "    xs = []\n",
    "    for entry in tmp:\n",
    "        res = str(entry).strip('][').split(', ') \n",
    "        res = ' '.join(res)\n",
    "        xs.append(res)\n",
    "        \n",
    "    df['processed_text'] = xs\n",
    "#     display(df)\n",
    "    return df\n",
    "\n",
    "sampled_data = create_text_column(sampled_data, 'review_body_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataframe Length 1150147,  Test Dataframe Length 287537\n"
     ]
    }
   ],
   "source": [
    "def create_train_test_data(df):\n",
    "    \n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    for row in df.itertuples(index=False):\n",
    "        xs.append(row.processed_text)\n",
    "        ys.append(row.label)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        xs, ys, test_size=0.20, random_state = 0)\n",
    "    \n",
    "    print(\"Train Dataframe Length {},  Test Dataframe Length {}\".format(len(X_train), len(X_test)))\n",
    "    \n",
    "    train_df = pd.DataFrame(list(zip(X_train, y_train)), \n",
    "               columns =['processed_text', 'label']) \n",
    " \n",
    "    test_df = pd.DataFrame(list(zip(X_test, y_test)), \n",
    "               columns =['processed_text', 'label']) \n",
    "    \n",
    "    return train_df, test_df \n",
    "    \n",
    "    \n",
    "    \n",
    "train_df, test_df = create_train_test_data(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_tuples(df):\n",
    "    \n",
    "    InputExample = namedtuple('InputExample', ['text', 'category_index'])\n",
    "    \n",
    "    data = []\n",
    "    for row in df.itertuples(index=False):\n",
    "        data.append(InputExample(text=row.processed_text, category_index=row.label))\n",
    "        \n",
    "    return data\n",
    "\n",
    "# tf_tuples =  build_tf_tuples(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples: List[Tuple[str, int]],tokenizer, max_length=512,):\n",
    "    \"\"\"\n",
    "    Loads data into a tf.data.Dataset for finetuning a given model.\n",
    "    Args:\n",
    "        examples: List of tuples representing the examples to be fed\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum string length\n",
    "    Returns:\n",
    "        a ``tf.data.Dataset`` containing the condensed features of the provided sentences\n",
    "    \"\"\"\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "    InputFeatures = namedtuple('InputFeatures', ['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default\n",
    "        )\n",
    "\n",
    "        # input ids = token indices in the tokenizer's internal dict\n",
    "        # token_type_ids = binary mask identifying different sequences in the model\n",
    "        # attention_mask = binary mask indicating the positions of padded tokens so the model does not attend to them\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.category_index\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensors(configs, tuples, num_labels):\n",
    "    \n",
    "    BATCH_SIZE = 16\n",
    "    num_examples = len(tuples)\n",
    "    TO_FINETUNE = configs['bert_model_type']\n",
    "    config = BertConfig.from_pretrained(TO_FINETUNE, num_labels=num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained(TO_FINETUNE)\n",
    "    \n",
    "    # Make the CPU do all data pre-processing steps, not the GPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        train_data = convert_examples_to_tf_dataset(tuples, tokenizer)\n",
    "        \n",
    "        train_data = train_data.shuffle(buffer_size=num_examples, reshuffle_each_iteration=True) \\\n",
    "                               .batch(BATCH_SIZE) \\\n",
    "                               .repeat(-1)\n",
    "        \n",
    "    return train_data\n",
    "\n",
    "# tf_train_data = generate_tensors(configs, tf_tuples, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1403 steps\n",
      "Epoch 1/3\n",
      " 782/1403 [===============>..............] - ETA: 4:40:36 - loss: 1.3886 - accuracy: 2.8338"
     ]
    }
   ],
   "source": [
    "def train_model_with_new_layer_classes(configs, global_vars, train_data, tuples, num_labels):\n",
    "    \n",
    "    EPOCHS = configs['bert_epochs']\n",
    "    BATCH_SIZE = configs['bert_batch_size']\n",
    "    TO_FINETUNE = configs['bert_model_type']\n",
    "    num_examples = len(tuples)\n",
    "\n",
    "    config = BertConfig.from_pretrained(TO_FINETUNE, num_labels=num_labels)\n",
    "    model = TFBertForSequenceClassification.from_pretrained(TO_FINETUNE, config=config)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-05, epsilon=1e-08)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalCrossentropy(name='accuracy')\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[metric])\n",
    "\n",
    "    train_steps = num_examples // BATCH_SIZE\n",
    "\n",
    "    model.fit(train_data, \n",
    "              epochs=EPOCHS,\n",
    "              steps_per_epoch=train_steps,\n",
    "#               verbose=2,\n",
    "              callbac ks=[tensorboard_callback]\n",
    "             )\n",
    "    \n",
    "    global_vars['bert_tf_model'] = model\n",
    "    global_vars['tensorboard_callback'] = tensorboard_callback\n",
    "    configs['log_dir'] = log_dir\n",
    "    \n",
    "    print('Fitting Data to Pre-trained Model')\n",
    "    \n",
    "    return global_vars, configs\n",
    "\n",
    "global_vars, configs = train_model_with_new_layer_classes(configs, global_vars, tf_train_data,tf_tuples,num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Save Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model, path):\n",
    "    \n",
    "    model.save_weights(path)\n",
    "    \n",
    "# os.mkdir('model/')\n",
    "save_model_weights(global_vars['bert_tf_model'], 'model/bert_tf_model_uncased_amazon_reviews_fine_tuned.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \n",
    "    model.save_pretrained(save_directory=path)\n",
    "    \n",
    "    \n",
    "# model = global_vars['bert_tf_model']\n",
    "\n",
    "os.mkdir(model_path)\n",
    "save_model(global_vars['bert_tf_model'], configs['model_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing model to Tar File\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def tar_model(model_path):\n",
    "    print('Compressing model to Tar File')\n",
    "    with tarfile.open('model/model.tar.gz', mode='w:gz') as archive:\n",
    "        archive.add(model_path, recursive=True)\n",
    "\n",
    "tar_model(configs['model_path'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "Only use this if you're loading the pre-trained fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  23070     \n",
      "=================================================================\n",
      "Total params: 109,505,310\n",
      "Trainable params: 109,505,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def load_model(global_vars, model_path):\n",
    "    \n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-05, epsilon=1e-08)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalCrossentropy(name='accuracy')\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[metric])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    global_vars['bert_tf_model'] = model\n",
    "    return global_vars\n",
    "    \n",
    "global_vars =  load_model(global_vars, configs['model_path'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(configs, model, df, labels_mapping):\n",
    "    \n",
    "    TO_FINETUNE = configs['bert_model_type']\n",
    "    config = BertConfig.from_pretrained(TO_FINETUNE)\n",
    "    tokenizer = BertTokenizer.from_pretrained(TO_FINETUNE)\n",
    "    TEST_STEPS = 100\n",
    "    y_preds = []\n",
    "    y = []\n",
    "    inputs = []\n",
    "    num_labels = len(labels_mapping)\n",
    "    tf_test_tuples =  build_tf_tuples(df)\n",
    "    tf_test_data = generate_tensors(configs, tf_test_tuples, num_labels)\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    log_dir = \"logs/eval/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_callback)\n",
    "\n",
    "    test_history = model.evaluate(tf_test_data,\n",
    "                             steps=TEST_STEPS,\n",
    "                              callbacks=callbacks,\n",
    "                                 verbose=3)\n",
    "    \n",
    "\n",
    "            \n",
    "    return test_history# y, y_preds\n",
    "\n",
    "\n",
    "test_history = evaluate_model(configs, global_vars['bert_tf_model'], test_df, labels_mapping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Attemp at parallizing TF, need to Use RAY to do this...\n",
    "\n",
    "# def encode_predict(x):\n",
    "#     data =  tokenizer.encode_plus(x, return_tensors=\"tf\", max_length=512)\n",
    "# #     print(data)\n",
    "#     outputs = model(data)\n",
    "#     classification_scores = outputs[0]\n",
    "#     lab = labels_mapping[np.argmax(classification_scores)]\n",
    "#     return lab\n",
    "\n",
    "# def predict_process(df, q, model, tokenizer):\n",
    "#     print('Running Process')    \n",
    "# #     TO_FINETUNE = configs['bert_model_type']\n",
    "# #     config = BertConfig.from_pretrained(TO_FINETUNE)\n",
    "# #     tokenizer = BertTokenizer.from_pretrained(TO_FINETUNE)\n",
    "#     y, y_preds = [], []\n",
    "#     cnt = 0\n",
    "#     print('Rows To compute {}'.format(df.shape[0]))\n",
    "# #     df['lab_pred']= df['processed_text'].apply(encode_predict)\n",
    "        \n",
    "#     for idx,row in df.iterrows():\n",
    "#         test_data = tokenizer.encode_plus(row['processed_text'], return_tensors=\"tf\", max_length=512)\n",
    "#         print(idx)\n",
    "#         outputs = model(test_data)\n",
    "#         classification_scores = outputs[0]\n",
    "#         df['pred_label'] = labels_mapping[np.argmax(classification_scores)]\n",
    "\n",
    "#         if cnt % 1 == 0:\n",
    "#             print('Step  {} of {}'.format(cnt, sample_size))\n",
    "#         cnt += 1\n",
    "    \n",
    "#     queue.put(df)\n",
    "\n",
    "\n",
    "# def evaluate_data_for_confusion_matrix_parallel(configs, model, df, labels_mapping):\n",
    "\n",
    "#     TO_FINETUNE = configs['bert_model_type']\n",
    "#     tokenizer = BertTokenizer.from_pretrained(TO_FINETUNE)\n",
    "    \n",
    "    \n",
    "#     df = df.sample(100)\n",
    "#     print('Starting Multicore inferencing Dataset Size {}'.format(df.shape[0]))\n",
    "#     cores = mp.cpu_count()\n",
    "#     df_split = np.array_split(df, 1)\n",
    "# #     print(len(df_split))\n",
    "#     jobs = []\n",
    "    \n",
    "#     q = mp.Queue()\n",
    "#     processes = []\n",
    "#     rets = []\n",
    "#     for i in range(0,1):\n",
    "#         print(i)\n",
    "#         p = mp.Process(target=predict_process, args=(df_split[i], q, model, tokenizer))\n",
    "#         processes.append(p)\n",
    "#         p.start()\n",
    "    \n",
    "#     for p in processes:\n",
    "#         ret = q.get() # will block\n",
    "#         rets.append(ret)\n",
    "        \n",
    "#     for p in processes:\n",
    "#         p.join()\n",
    "        \n",
    "# #     pool = Pool(cores)\n",
    "# #     pool_results = pool.map(predict_process, df_split)\n",
    "# #     pool.close()\n",
    "# #     pool.join()\n",
    "    \n",
    "#     parts = pd.concat(rets, axis=0)\n",
    "#     print('Finished Multicore inferencing Dataset Size {}'.format(parts.shape[0]))\n",
    "\n",
    "#     return parts\n",
    "\n",
    "\n",
    "# #run this with the test data\n",
    "# test_pred_df = evaluate_data_for_confusion_matrix(configs, \n",
    "#                                                   global_vars['bert_tf_model'], \n",
    "#                                                   test_df, \n",
    "\n",
    "#                                                   labels_mapping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  5 of 287537\n",
      "Eval Classification Accuracy: 60.0\n",
      "Step  10 of 287537\n",
      "Eval Classification Accuracy: 60.0\n",
      "Step  15 of 287537\n",
      "Eval Classification Accuracy: 46.666666666666664\n",
      "Step  20 of 287537\n",
      "Eval Classification Accuracy: 50.0\n",
      "Step  25 of 287537\n",
      "Eval Classification Accuracy: 52.0\n",
      "Step  30 of 287537\n",
      "Eval Classification Accuracy: 53.333333333333336\n",
      "Step  35 of 287537\n",
      "Eval Classification Accuracy: 54.285714285714285\n",
      "Step  40 of 287537\n",
      "Eval Classification Accuracy: 55.00000000000001\n",
      "Step  45 of 287537\n",
      "Eval Classification Accuracy: 55.55555555555556\n",
      "Step  50 of 287537\n",
      "Eval Classification Accuracy: 54.0\n",
      "Step  55 of 287537\n",
      "Eval Classification Accuracy: 58.18181818181818\n",
      "Step  60 of 287537\n",
      "Eval Classification Accuracy: 60.0\n",
      "Step  65 of 287537\n",
      "Eval Classification Accuracy: 61.53846153846154\n",
      "Step  70 of 287537\n",
      "Eval Classification Accuracy: 58.57142857142858\n",
      "Step  75 of 287537\n",
      "Eval Classification Accuracy: 56.00000000000001\n",
      "Step  80 of 287537\n",
      "Eval Classification Accuracy: 56.25\n",
      "Step  85 of 287537\n",
      "Eval Classification Accuracy: 57.647058823529406\n",
      "Step  90 of 287537\n",
      "Eval Classification Accuracy: 57.77777777777777\n",
      "Step  95 of 287537\n",
      "Eval Classification Accuracy: 57.89473684210527\n",
      "Step  100 of 287537\n",
      "Eval Classification Accuracy: 59.0\n",
      "Step  105 of 287537\n",
      "Eval Classification Accuracy: 58.0952380952381\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "def evaluate_data_for_confusion_matrix(configs, model, df, labels_mapping):\n",
    "    \n",
    "    \n",
    "    \n",
    "    TO_FINETUNE = configs['bert_model_type']\n",
    "    config = BertConfig.from_pretrained(TO_FINETUNE)\n",
    "    tokenizer = BertTokenizer.from_pretrained(TO_FINETUNE)\n",
    "    \n",
    "    inference_pipeline = TextClassificationPipeline(model=model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='tf',\n",
    "                                                device=-1) # -1 is CPU, >= 0 is GPU\n",
    "    \n",
    "    y, y_preds = [], []\n",
    "    cnt = 0\n",
    "    for idx,row in df.iterrows():\n",
    "#         print(idx)\n",
    "#         test_data = tokenizer.encode_plus(row['processed_text'], return_tensors=\"tf\", max_length=512)\n",
    "#         outputs = model(test_data)\n",
    "#         print(inference_pipeline(row['processed_text']))\n",
    "        preds = inference_pipeline(row['processed_text'])\n",
    "        pred_lab = int(preds[0]['label'].replace('LABEL_',''))\n",
    "#         classification_scores = \n",
    "        y_preds.append(labels_mapping[pred_lab])\n",
    "        y.append(labels_mapping[row['label']])\n",
    "#         print(y_preds)\n",
    "#         print(y)\n",
    "        cnt += 1\n",
    "        if cnt % 5 == 0:\n",
    "            \n",
    "            total_predictions = len(y)\n",
    "            correct_predictions = 0\n",
    "            for i in range(0,len(y_preds)):\n",
    "                if y_preds[i] == y[i]:\n",
    "                    correct_predictions += 1\n",
    "            classification_accuracy = correct_predictions / total_predictions * 100.0\n",
    "            print('Step  {} of {}'.format(cnt, df.shape[0]))\n",
    "            print('Eval Classification Accuracy: {}'.format(classification_accuracy))\n",
    "                \n",
    "            \n",
    "    return y, y_preds\n",
    "\n",
    "#run this with the test data\n",
    "test_pred_df = evaluate_data_for_confusion_matrix(configs, \n",
    "                                                  global_vars['bert_tf_model'], \n",
    "                                                  test_df, \n",
    "                                                  labels_mapping )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload BERT Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 BERT Data Path s3://demos-amazon-reviews/models/bert/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def upload_bert_to_s3(configs, global_vars, model_tar):\n",
    "    \n",
    "    \n",
    "    models_dir = configs['models_dir']\n",
    "    prefix = 'bert'\n",
    "    model_s3_filename = 'model.tar.gz'\n",
    "    \n",
    "    s3_bucket = global_vars['s3_bucket']\n",
    "    sess = global_vars['sess']\n",
    "    bucket = global_vars['s3_bucket']\n",
    "   \n",
    "    \n",
    "\n",
    "    model_file_s3 = '{}/{}/{}'.format(models_dir, prefix, model_s3_filename)\n",
    "    s3_bucket.upload_file(model_tar, model_file_s3)   \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    s3_bert_model = 's3://{}/{}/{}/model.tar.gz'.format(configs['bucket_name'], models_dir, prefix)\n",
    "    \n",
    "    configs['s3_bert_model'] = s3_bert_model\n",
    "   \n",
    "    print('S3 BERT Data Path {}'.format(s3_bert_model))\n",
    "   \n",
    "\n",
    "    return configs\n",
    "\n",
    "model_tar = 'model/model.tar.gz'\n",
    "configs = upload_bert_to_s3(configs, global_vars, model_tar)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings from TF-IDF, Word2Vec, BERT\n",
    "\n",
    "In this notebook we have explored a sample of the dataset, and then applied a variety of text analysis methods to understand the application of natural language processsing techniques to perform classification and prediction of an Amazon Review product category.\n",
    "\n",
    "What we've learnt is that depending on the method (TF-IDF, Word Embeddings va blazingtext, and BERT), the speed of training and inferencing as we increase the complexity of the method (e.g. BERT being the most complex), however, we do see an increase in classifcation score as a result.\n",
    "\n",
    "With our 1% sample which has been taken from a fair distribution of the total 140 million reviews, we can see the performance for multi-class classification performs as follows:\n",
    "\n",
    "- TF-IDF  - 65%\n",
    "- Word Embeddings - 75%\n",
    "- BERT - 82%\n",
    "\n",
    "Let's consider the computational time taken to train and then perform inferencing (local model):\n",
    "\n",
    "- TF-IDF - Training: 10 minutes. Inferencing: < ~1ms\n",
    "- Word-Embeddings - This was using SageMaker distributed mode so cannot compare\n",
    "- BERT - Training: 18 hours. Inferencing: ~15 seconds (per sample)\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- BERT represents the state-of-the-art technique for performing NLP tasks such as sequence classification, however due to the complexity of the model, the training and the inferening is costly w.r.t time and computational resources. Whilst this was performed on on local mode, we should be able to scale up the processing time for training, the inferencing time will be similar to the response of that of the local mode, due to inferencing is not a distributed task.\n",
    "- TF-IDF provided adequate results, however, due to two part approach to TF-IDF - first undsupervised training to identify frequency and document frequency scores, then using these scores to train a classifier - the amount of heavy lifting is substantial. There was a significant amount of data preparation and tuning that was required in order to acheive adequate results. Also, TF-IDF cannot capture the semantics of the words, thus makes it difficult to use the outputs for other tasks. Furthermore, as we had to train a classifier on the back of the TF-IDF scores, it would be significantly slower (and require extensive computational resources), if we were to move to the full dataset.\n",
    "- Word Embeddings: Word Embeddings provided the best of both worlds, it allows for the semantics of the tokens to persist within the training cycles (we can set the size of the n-grams we wish to keep), plus it provides acceptable results (> 75%) for classifying reviews based on their product_category\n",
    "\n",
    "Going forward, we'll take the Word Embeddings Model and apply it to the larger corpus of reviews.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
