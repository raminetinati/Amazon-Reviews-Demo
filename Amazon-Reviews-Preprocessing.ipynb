{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Customer Reviews - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1583290770311_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-61-146.ec2.internal:20888/proxy/application_1583290770311_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-59-254.ec2.internal:8042/node/containerlogs/container_1583290770311_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "import boto3\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "from pyspark.sql.functions import col, udf\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pyspark.sql.types import StructType, ArrayType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews = spark.read.parquet(\"s3://amazon-reviews-pds/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MIN_SENTENCE_LENGTH_IN_CHARS = 10\n",
    "MAX_SENTENCE_LENGTH_IN_CHARS = 5000\n",
    "COMPREHEND_BATCH_SIZE = 25\n",
    "NUMBER_OF_BATCHES = 4\n",
    "ROW_LIMIT = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824cf5ea43f34ddd89709f5e7d0ab162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = reviews \\\n",
    "  .distinct() \\\n",
    "  .filter(\"marketplace = 'US'\") \\\n",
    "  .withColumn('body_len', F.length('review_body')) \\\n",
    "  .filter(F.col('body_len') > MIN_SENTENCE_LENGTH_IN_CHARS) \\\n",
    "  .filter(F.col('body_len') < MAX_SENTENCE_LENGTH_IN_CHARS) \\\n",
    "#   .limit(ROW_LIMIT)\n",
    "\n",
    "record_count = df.count()\n",
    "print('Total Record Processing: {}'.format(record_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "  .repartition(int(record_count/(NUMBER_OF_BATCHES*COMPREHEND_BATCH_SIZE)))\\\n",
    "  .sortWithinPartitions(['review_id'], ascending=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Review Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenizer\n",
    "def word_tokenize(x):\n",
    "    lowerW = x.lower()\n",
    "    words = lowerW.split()\n",
    "    return words\n",
    "\n",
    "def filter_punctuation(x):\n",
    "    list_punct=list(string.punctuation)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in x]\n",
    "    return stripped\n",
    "\n",
    "##load stopwords - can't use NLTK to do this...\n",
    "def remove_stopwords(x):\n",
    "    stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    stopW = [word for word in x if word not in stopwords and word !='']\n",
    "    return stopW\n",
    "\n",
    "# text = \"testing this, because i am someone that need's food # $ %\"\n",
    "def preprocess_text_to_tokens(x):\n",
    "    tokens = word_tokenize(x)\n",
    "    tokens = filter_punctuation(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text = udf(lambda row: preprocess_text_to_tokens(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('review_body_processed', preprocess_text(col('review_body')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# df_new[['review_body','review_body_processed']].show()\n",
    "\n",
    "+--------------------+---------------------+\n",
    "|         review_body|review_body_processed|\n",
    "+--------------------+---------------------+\n",
    "|I have this watch...| [watch, believe, ...|\n",
    "|I use this watch ...| [use, watch, busi...|\n",
    "|Bought this watch...| [bought, watch, a...|\n",
    "|My watch was dead...| [watch, dead, arr...|\n",
    "|It is good watch ...| [good, watch, rec...|\n",
    "|The watches I bou...| [watches, bought,...|\n",
    "|this is a very ni...| [nice, time, piec...|\n",
    "|The product is as...| [product, expecte...|\n",
    "|I saw either this...| [saw, either, wat...|\n",
    "|I read several re...| [read, several, r...|\n",
    "|Wife enjoys weari...| [wife, enjoys, we...|\n",
    "|This watch looks ...| [watch, looks, gr...|\n",
    "|Is a excellent pr...| [excellent, produ...|\n",
    "|Thanks amazon for...| [thanks, amazon, ...|\n",
    "|Just Amazing, thi...| [amazing, product...|\n",
    "|very nice watch l...|  [nice, watch, love]|\n",
    "|If you are search...| [searching, rugge...|\n",
    "|Not at ALL what w...|           [expected]|\n",
    "|         Looks great|       [looks, great]|\n",
    "|This is my first ...| [first, seiko, au...|\n",
    "+--------------------+---------------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Month Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date to string with format yyyy-mm\n",
    "func_to_str =  udf (lambda x: datetime.strftime(x, '%Y-%m'))\n",
    "\n",
    "#apply the udf to the df\n",
    "df = df.withColumn('review_date_str', func_to_str(col('review_date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d3ac2912764576935f5ae310ada16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://localhost:8998/sessions/4/statements/15 with error payload: \"requirement failed: Session isn't active.\"\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "  .write \\\n",
    "  .partitionBy('review_date_str') \\\n",
    "  .mode('overwrite') \\\n",
    "  .parquet(\"s3://demos-amazon-reviews/preprocessed_reviews/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
